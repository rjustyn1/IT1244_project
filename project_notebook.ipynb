{"cells":[{"cell_type":"markdown","metadata":{"id":"7YObopEjLNQm"},"source":["## <span style = \"font-family: Cambria\">**Task: Classification model to accurately distinguish benign and malignant tumours**</span>\n","\n","Here are some of the references that we used when approaching this task:\n","1. Nguyen, A., Yosinski, J., & Clune, J. (2015). Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 427-436).\n","\n","2. Anaya-Isaza, A., Mera-Jiménez, L., Verdugo-Alejo, L., & Sarasti, L. (2023). Optimizing MRI-based brain tumour classification and detection using AI: A comparative analysis of neural networks, transfer learning, data augmentation, and the cross-transformer network. European journal of radiology open, 10, 100484. https://doi.org/10.1016/j.ejro.2023.100484\n","\n","3. Arsa, Dewa Made Sri, and Anak Agung Ngurah Hary Susila. “VGG16 in Batik Classification Based on Random Forest.” In 2019 International Conference on Information Management and Technology (ICIMTech), 295–99. Jakarta/Bali, Indonesia: IEEE, 2019. https://doi.org/10.1109/ICIMTech.2019.8843844.\n","\n","4. Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. “Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.” In 2017 IEEE International Conference on Computer Vision (ICCV), 618–26. Venice: IEEE, 2017. https://doi.org/10.1109/ICCV.2017.74.\n","\n","\n","\n","Some limitations we wish to resolve:\n","1. Nguyen et al. (2015) touched on the unpredictable behaviour of AlexNet. However, due to the blackbox nature of CNN, we could explain the rationale behind the unpredictable nature. Hence, we wish to improve on the interpretability of CNN in image classifications through the use of GradCAM.\n","2. Anaya-Isaza et al. explored geometric flipping image augmentation in their work. We wish to extend and further explore other image augmentation techniques with Keras ImageDataGenerator to improve robustness of training with limited datasets.\n","3. Arsa et al. explored the use of VGG16-Random Forest for Batik Classification, we wish to extend on it by exploring the CNN-XGBoost pairing in our image classification due to the strong regularisation potential of XGBoost."]},{"cell_type":"markdown","metadata":{"id":"JfxcRjqfLNQo"},"source":["### <span style = \"font-family: Cambria\">Content explorer</span>"]},{"cell_type":"markdown","metadata":{"id":"uUzy-P_ULNQp"},"source":["In order, this notebook is categorised into:\n","1. [Helper functions](#helper-functions)\n","- General functions i.e., plotting , image augmentation\n","- Preprocessing functions\n","- GradCAM Helper functions\n","2. [Data Preparation](#prepare)\n","3. [Baseline model](#baseline)\n","4. [VGG16-ANN model](#ann)\n","5. [VGG16-XgBoost model](#xgboost)"]},{"cell_type":"markdown","metadata":{"id":"0br1SZSZLNQp"},"source":["### <span style = \"font-family: Cambria\">Installing dependencies and imports</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":205291,"status":"ok","timestamp":1712472450560,"user":{"displayName":"Easily forgettable youtube commenter","userId":"07919466590426017184"},"user_tz":-480},"id":"_azGeHOGLNQp","outputId":"c6166ee6-7150-4408-fff8-20c8be940282"},"outputs":[],"source":["#Uncomment and run to install relevant packages\n","#Note that this notebook may not run for newer keras versions\n","'''\n","!pip install numpy==1.24.3\n","!pip install pandas==1.5.3\n","!pip install matplotlib==3.6.3\n","!pip install imutils==0.5.4\n","!pip install opencv-python==4.9.0.80\n","!pip install ImageHash==4.3.1\n","!pip install tqdm==4.65.0\n","!pip install scikit-learn==1.4.1.post1\n","!pip install scikit-image==0.22.0\n","!pip install xgboost==2.0.3\n","!pip install scikeras==0.12.0\n","!pip install keras==2.12.0\n","!pip install tensorflow==2.12.1\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gByjNX0CLNQq"},"outputs":[],"source":["from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import imutils\n","import cv2\n","import os\n","import random\n","import pickle\n","import imagehash\n","from PIL import Image\n","from tqdm import tqdm\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n","from sklearn.decomposition import PCA\n","from sklearn.svm import SVC\n","from skimage import color\n","from xgboost import XGBoost\n","from scikeras.wrappers import KerasClassifier\n","\n","\n","import keras\n","from keras.applications import VGG16\n","from keras.models import Model, Sequential\n","from keras.layers import Input, Dense, Activation, Flatten, Dropout, Conv2D, MaxPool2D, BatchNormalization, MaxPooling2D\n","from keras.optimizers import Adam, SGD\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.metrics import Precision, Recall, AUC\n","\n","import sklearn.metrics\n","from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, f1_score, accuracy_score, precision_score, recall_score"]},{"cell_type":"markdown","metadata":{"id":"oJ3H5AJoLNQq"},"source":["# <a id = \"helper-functions\"></a> <span style = \"font-family: Cambria\">**Helper functions**</span>"]},{"cell_type":"markdown","metadata":{"id":"ywvQBhieLNQq"},"source":["## <span style = \"font-family: Cambria\">General functions</span>"]},{"cell_type":"markdown","metadata":{"id":"ewUfqDweLNQr"},"source":["### <span style = \"font-family: Cambria\">1. Finding images from filesystem</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NY-8kSgKLNQr"},"outputs":[],"source":["def fileList(source):\n","    '''\n","    Returns list of filepaths with .jpg extensions from root\n","    '''\n","    matches = []\n","    for root, _, filenames in os.walk(source):\n","        for filename in filenames:\n","            if filename.endswith((\".jpg\")):\n","                matches.append(os.path.join(root, filename))\n","    return matches"]},{"cell_type":"markdown","metadata":{"id":"N36b4dvfLNQr"},"source":["### <span style = \"font-family: Cambria\">2. Plot helper functions</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86jKRZtBLNQr"},"outputs":[],"source":["def grid_plot(images, dim = (10,10)):\n","    '''\n","    Display the images in a grid.\n","\n","    Args:\n","        image (nd.array): Input image (numpy array).\n","    '''\n","    rows, cols = dim\n","    r_c, c_c = 0, 0\n","    fig = plt.figure(figsize=(1.7*rows, 1.3*cols))\n","\n","    #Looping over for i = rows x cols\n","    for i, image in enumerate(images[:(rows*cols)]):\n","        fig.add_subplot(rows, cols, i+1)\n","        plt.imshow(image)\n","        plt.tight_layout()\n","        plt.axis(False)\n","\n","        r_c+=1\n","        c_c+=1\n","\n","    plt.subplots_adjust(hspace=0, wspace=0)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zqxJ2QA9LNQr"},"source":["### <span style = \"font-family: Cambria\">3. Shuffling 2 arrays</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztMOqDdgLNQr"},"outputs":[],"source":["def shuffling(a, b):\n","    \"\"\"\n","    Shuffle 2 numpy arrays in the same order\n","\n","    Args:\n","        a (nd.array)\n","        b (nd.array)\n","    \"\"\"\n","    p = np.random.permutation(len(a))\n","    return a[p], b[p]"]},{"cell_type":"markdown","metadata":{"id":"U2aet-DfLNQr"},"source":["### <span style = \"font-family: Cambria\">4. Image augmentation</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lh_ewA7dLNQr"},"outputs":[],"source":["train_datagen = ImageDataGenerator(\n","    samplewise_center = True,\n","    samplewise_std_normalization = True,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    fill_mode=\"nearest\")\n","\n","validation_datagen = ImageDataGenerator(\n","    samplewise_center = True,\n","    samplewise_std_normalization = True)\n","\n","# particularly for GridSearchCV since we cannot pass in a generator object\n","def augment_images(X):\n","    '''\n","    Apply augmentation strategies to images.\n","\n","    Args:\n","        X (nd.array): Input data of image arrays\n","\n","    Returns:\n","        new_imgs (nd.array): Numpy array of augmented images\n","    '''\n","\n","    new_imgs = []\n","    for image in X:\n","        imba_augmenter = ImageDataGenerator()\n","\n","        # augment the images to make models robust\n","        augment_param = {\n","                \"theta\": random.randint(-40, 40),\n","                \"shear\": random.uniform(-0.2, 0.2),\n","                \"tx\": random.uniform(-0.2, 0.2),\n","                \"ty\": random.uniform(-0.2, 0.2),\n","                \"zx\": random.uniform(-0.2, 0.2),\n","                \"zy\": random.uniform(-0.2, 0.2),\n","                \"flip_horizontal\": True,\n","                \"flip_vertical\": bool(random.randint(0, 1)),\n","            }\n","\n","        new_img = imba_augmenter.apply_transform(image, augment_param)\n","\n","        # standardise the data\n","        new_img = imba_augmenter.standardize(new_img)\n","        new_imgs.append(new_img)\n","\n","    return np.array(new_imgs)"]},{"cell_type":"markdown","metadata":{"id":"kAh3ya-ZLNQs"},"source":["### <span style = \"font-family: Cambria\">5. Initialising VGG16 TF model</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4h0XRp8-LNQs","outputId":"a9613a9a-3ca7-448e-bd9e-d5502c272213"},"outputs":[],"source":["def create_vgg16tf():\n","    \"\"\"\n","    Creates a VGG16 transfer learning model with a customer classifier\n","\n","    Returns:\n","        model (keras.Model)\n","    \"\"\"\n","\n","    inputs = tf.keras.Input(shape=(224, 224, 3))\n","    x = VGG16(input_tensor=inputs ,weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","    x.trainable = False\n","\n","    x = Flatten()(x.output)\n","    x = Dense(256, activation=\"relu\")(x)\n","    x = Dropout(rate=0.4)(x)\n","    x = Dense(2, activation=\"softmax\")(x)\n","    model = tf.keras.Model(inputs, x)\n","\n","    return model\n","\n","create_vgg16tf().summary()"]},{"cell_type":"markdown","metadata":{"id":"S7Ez7aXxLNQs"},"source":["### <span style = \"font-family: Cambria\">6. Loading model with best weights</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGsbcnydLNQs"},"outputs":[],"source":["def load_best_model(path, create_model_fn):\n","    \"\"\"\n","    Load model with best weights.\n","\n","    Args:\n","        path (str): Filepath of .h5 file containing the best weights.\n","        create_model_fn (function): Function to create empty model, should be same model architecture as weights file\n","    \"\"\"\n","\n","    model = create_model_fn()\n","    model.load_weights(path)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"0nXGjCV2LNQs"},"source":["### <span style = \"font-family: Cambria\">7. Additional metrics for VGG16-ANN model</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u98cNMRsLNQs"},"outputs":[],"source":["def added_metrics(pred_prob, grd_truth):\n","    \"\"\"\n","    Custom metrics function for VGG16-ANN\n","\n","    Args:\n","        pred_prob (nd.array): Predictions of model (n x 2)\n","        grd_truth (nd.array): Labels (n x 2)\n","\n","    Returns:\n","        (list) List of metrics\n","    \"\"\"\n","    pred = np.argmax(pred_prob, axis = 1)\n","    grd_truth = np.argmax(grd_truth, axis = 1)\n","    tn, fp, fn, tp = confusion_matrix(pred, grd_truth).ravel()\n","\n","    accuracy = (tp+tn)/(fp+fn+tp+tn)\n","    precision = tp/(tp+fp)\n","    recall = tp/(tp+fn)\n","\n","    return [tn, tp, fn, fp, precision, accuracy, recall]\n"]},{"cell_type":"markdown","metadata":{"id":"f_M3NyjnLNQs"},"source":["## <a id = \"preprocess\"></a><span style = \"font-family: Cambria\">Dataset processing functions</span>"]},{"cell_type":"markdown","metadata":{"id":"MQIQYvvWLNQs"},"source":["### <span style = \"font-family: Cambria\">1. Removing near duplicates</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P00bQ9UPLNQs"},"outputs":[],"source":["def display_images_in_row(image_paths, labels=None, size=(5, 5)):\n","    \"\"\"\n","      Plot images in a row\n","\n","      Args:\n","          image_paths (list): List of image paths\n","          labels (list): Title for plots\n","          size (tuple): Size of figure\n","    \"\"\"\n","    num_images = len(image_paths)\n","    if num_images == 0:\n","        return\n","\n","    fig, axes = plt.subplots(1, num_images, figsize=(num_images * size[0]/3, size[1]))\n","\n","    for i, (image_path, label) in enumerate(zip(image_paths, labels)):\n","        image = Image.open(image_path)\n","        axes[i].imshow(image)\n","        axes[i].axis('off')\n","        if labels:\n","            axes[i].set_title(label)\n","\n","    plt.show()\n","\n","def find_near_duplicates(image_paths, threshold=5, verbose=0):\n","      \"\"\"\n","      Utilises phash from imagehash library to remove duplicates and near duplicates from dataset.\n","      Shows number of duplicated and unique images from each filepaths\n","\n","      Args:\n","          image_paths (list): List of image paths\n","          threshold (int or float): Threshold used to determine near duplicates (minimum hamming difference\n","                                    of image hashs to be considered unique)\n","          verbose (int): 0 - No plots\n","                         1 - Show rows of duplicated images\n","\n","      Returns:\n","          unique_image_paths (list): List of unique image paths\n","      \"\"\"\n","\n","      # Dictionary to store hashes for each image\n","      image_hashes = {}\n","      # List to store near-duplicate pairs\n","      near_dups = {}\n","      # Iterate over all image paths\n","      for image_path in image_paths:\n","          # Load image\n","          image = Image.open(image_path).convert('L')  # Convert to grayscale\n","          # Calculate perceptual hash\n","          image_hash = imagehash.phash(image)\n","          # Store hash in dictionary\n","          image_hashes[image_path] = image_hash\n","      # Compare hashes to find near-duplicates\n","      for path1, hash1 in image_hashes.items():\n","        #if dup already considered avoid overlap\n","        if path1 in near_dups:\n","          continue\n","        list_of_dups = []\n","        dup_image_names=[]\n","        for path2, hash2 in image_hashes.items():\n","\n","          # If images are not the same and if the hamming distance of 2 images is less than threshold\n","          # This parts tracks the duplicated images\n","          if path1 != path2 and hash1 - hash2 < threshold:\n","            near_dups[path2]=hash2\n","            if len(list_of_dups) ==0:\n","              list_of_dups.append(path1)\n","              dup_image_names.append(os.path.basename(path1))\n","            list_of_dups.append(path2)\n","            dup_image_names.append(os.path.basename(path2))\n","\n","        if verbose == 1:\n","          if len(list_of_dups) >0:\n","            print(os.path.dirname(list_of_dups[0]))\n","            print(dup_image_names)\n","\n","      unique_image_paths = []\n","      tumour_count = 0\n","\n","      for path in image_paths:\n","        if path not in near_dups:\n","          unique_image_paths.append(path)\n","          tumour_count += 1\n","\n","      print(f\"Number of duplicated {os.path.basename(os.path.dirname(path))} tumours: {len(near_dups)}\")\n","      print(f\"Number of {os.path.basename(os.path.dirname(path))} tumours: {tumour_count}\")\n","      return unique_image_paths"]},{"cell_type":"markdown","metadata":{"id":"pkw_wGBELNQt"},"source":["### <span style = \"font-family: Cambria\">2. Crop image</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUolfWzLLNQt"},"outputs":[],"source":["def crop_img(img):\n","    \"\"\"\n","    Crops out majority of the background and centralised the brain\n","\n","    Args:\n","        img (nd.array)\n","\n","    Returns:\n","        new_imgs(nd.array): Cropped image resized to 224 pixels by 224 pixels\n","    \"\"\"\n","\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n","\n","    # Clearing noises\n","    thresh = cv2.threshold(gray, 20, 255, cv2.THRESH_BINARY)[1]\n","    thresh = cv2.erode(thresh, None, iterations=2)\n","    thresh = cv2.dilate(thresh, None, iterations=2)\n","\n","    # Finding contours and grab the main image\n","    contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    contours = imutils.grab_contours(contours)\n","    c = max(contours, key = cv2.contourArea)\n","\n","    # Finding boundaries\n","    left = tuple(c[c[:, :, 0].argmin()][0])\n","    right = tuple(c[c[:, :, 0].argmax()][0])\n","    top = tuple(c[c[:, :, 1].argmin()][0])\n","    bottom = tuple(c[c[:, :, 1].argmax()][0])\n","\n","    # New image cropping\n","    new_img = img[top[1] : bottom[1], left[0] : right[0]]\n","    new_img = cv2.resize(new_img, (224,224))\n","\n","    return new_img"]},{"cell_type":"markdown","metadata":{"id":"ZHAExSEXLNQt"},"source":["### <span style = \"font-family: Cambria\">3. Image mask</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZIyPfJ8LNQt"},"outputs":[],"source":["def mask(image):\n","    '''\n","\n","    Create a binary mask separating the brain from the background, postprocess and apply the brain mask to original.\n","\n","    Args:\n","        image (nd.array): Input image (numpy array).\n","\n","    Returns:\n","        masked_image (nd.array): Image with the mask applied.\n","    '''\n","\n","    # Apply thresholding to create a binary mask separating the brain from the background\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    blur = cv2.GaussianBlur(gray_image, (7,7), 0)\n","    normalized = cv2.normalize(blur, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","    _, thresh = cv2.threshold(normalized, 10, 255, cv2.THRESH_BINARY)\n","    kernel = np.ones((3,3), np.uint8)\n","    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n","\n","    # Postprocessing\n","    contours, _ = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    brain_mask = np.zeros_like(normalized)\n","    for cnt in contours:\n","        area = cv2.contourArea(cnt)\n","        if area > 500:  # threshold\n","            cv2.drawContours(brain_mask, [cnt], -1, 255, -1)\n","\n","    # Apply the brain mask to the original image\n","    masked_brain = cv2.bitwise_and(image, image, mask=brain_mask)\n","\n","    return masked_brain"]},{"cell_type":"markdown","metadata":{"id":"pLHykLfwLNQt"},"source":["### <span style = \"font-family: Cambria\">4. Image Augmentation for class imbalance</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QURC_grwLNQt"},"outputs":[],"source":["def augment_minority_class(train_X, train_y, verbose=0, seed=42):\n","    '''\n","    Augment only benign images in train X and from there, apppend to train X and y.\n","\n","    Args:\n","        train_X (nd.array): Input image (numpy array).\n","        train_y (nd.array): Class labels (numpy array).\n","\n","    Returns:\n","        train_X (nd.array): Input image with more benign images.\n","        train_y (nd.array): Class labels with more 0s.\n","    '''\n","\n","    random.seed(seed)\n","    _, counts = np.unique(train_y, return_counts = True)\n","    min_class = np.argmin(counts)\n","\n","    augment_count = max((max(counts)//min(counts)) - 1, 1) * min(counts)\n","\n","    # Separate benign images in train_X\n","    min_class_idx = np.argwhere(train_y==min_class).reshape(1,-1)[0]\n","    train_X_minclass = train_X[min_class_idx]\n","\n","    # Generator seed provides seeds for image augmentation based on parent seed\n","    generator_seed = [random.randint(0,10000) for _ in range(augment_count)]\n","    imba_augmenter = ImageDataGenerator()\n","\n","    new_imgs=[]\n","    for count, img in enumerate(train_X_minclass):\n","        if count >= augment_count:\n","            break\n","\n","        random.seed(generator_seed[count])\n","\n","        ### EDIT AUGMENT PARAMETERS HERE\n","        augment_param = {\n","            \"theta\": random.randint(-35, 35),\n","            \"shear\": random.uniform(0, 0.1),\n","            \"tx\": random.uniform(0.8, 1.3),\n","            \"ty\": random.uniform(0.8, 1.3),\n","            \"zx\": random.uniform(0.8, 1.3),\n","            \"zy\": random.uniform(0.8, 1.3),\n","            \"flip_horizontal\": True,\n","            \"flip_vertical\": bool(random.randint(0, 1)),\n","        }\n","\n","        new_img = imba_augmenter.apply_transform(img, augment_param)\n","        new_imgs.append(new_img)\n","\n","        # Verbose of 1 allows to see how is the augmentation like\n","        if verbose == 1:\n","            print(augment_param)\n","            plt.imshow(new_img)\n","            plt.show()\n","\n","        train_X = np.append(train_X, new_img[np.newaxis,:,:,:], axis=0)\n","        train_y = np.append(train_y, min_class)\n","        train_X, train_y = shuffling(train_X, train_y)\n","\n","    # Verbose of 2 provides grid plot of augmented images\n","    if verbose == 2:\n","        grid_plot(new_imgs)\n","\n","    return train_X, train_y\n","\n","#USAGE: augmented_images = augment_minority_class(train_X, train_y, verbose=1)\n","\n","\n","def augment_images_xgboost(input, num_of_images, verbose=0, seed=42):\n","    '''\n","    Slightly different augment function. Able to augment benign and malignant class separately\n","    with stated number of images\n","\n","    Args:\n","        input (nd.array): Input image (numpy array).\n","        num_of_images (int): Number of times to augment\n","\n","    Returns:\n","        new_imgs (nd.array): Input image with more benign images.\n","    '''\n","\n","    random.seed(seed)\n","    if not isinstance(input[0], np.ndarray) or not len(input[0].shape) >= 2:\n","        input = [cv2.imread(path) for path in input]\n","\n","    # Generator seed provides seeds for image augmentation based on parent seed\n","    generator_seed = [random.randint(0,10000) for _ in range(num_of_images)]\n","    imba_augmenter = ImageDataGenerator()\n","\n","    new_imgs = []\n","    for count in range(num_of_images):\n","        if count < len(input):\n","            index = count\n","        else:\n","            index = random.randint(0, len(input)-1)\n","\n","        img = input[index]\n","\n","        random.seed(generator_seed[count])\n","\n","        ### EDIT AUGMENT PARAMETERS HERE\n","        augment_param = {\n","            \"theta\": random.randint(-35,35),\n","            \"shear\": random.uniform(0, 0.1),\n","            \"zx\": random.uniform(0.8, 1.3),\n","            \"zy\": random.uniform(0.8, 1.3),\n","            \"flip_horizontal\": True,\n","            \"flip_vertical\": bool(random.randint(0, 1))\n","        }\n","\n","        new_img = imba_augmenter.apply_transform(img, augment_param)\n","\n","        # Verbose of 1 allows to see how is the augmentation like\n","        if verbose == 1:\n","            print(augment_param)\n","            plt.imshow(new_img)\n","            plt.show()\n","        new_imgs.append(new_img)\n","    return np.array(new_imgs)"]},{"cell_type":"markdown","metadata":{"id":"gnKOeqwpLNQt"},"source":["### <span style = \"font-family: Cambria\">5. Master preprocessing pipeline</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mL1Avqg6LNQt"},"outputs":[],"source":["def preprocessing(img_paths, verbose=0):\n","    '''\n","    Read the images from image paths and preprocess images using all the above preprocessing functions (1-4).\n","\n","    Args:\n","        img_paths (List): List of file paths\n","\n","    Returns:\n","        processed (List): List of numpy arrays of unique images cropped and masked.\n","    '''\n","    img_paths_no_dup = find_near_duplicates(img_paths, verbose=verbose)\n","    img_list = [cv2.imread(path) for path in img_paths_no_dup]\n","\n","    processed = []\n","    for img in img_list:\n","        img = mask(img)\n","        processed.append(crop_img(img))\n","\n","    return processed\n"]},{"cell_type":"markdown","metadata":{"id":"HmOaw3XeLNQt"},"source":["## <a id = \"explain\"></a><span style = \"font-family: Cambria\">GradCAM helper function</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSL25OBRLNQt"},"outputs":[],"source":["#Create gradcam model with input image connected with output of last pooling layer and output\n","\n","def grad_heatmap(model, img_bg_data, img_bg_label , layer_name_for_grad_cal, dim=(7,10)):\n","    \"\"\"\n","    Provides plots of GradCAM heatmap visualisations using grid_plot\n","    Code for computing GradCAM heatmap and visualising it is obtained from: https://keras.io/examples/vision/grad_cam/\n","\n","    GradCAM algorithm referenced from\n","        Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\n","        and Dhruv Batra. “Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.”\n","        In 2017 IEEE International Conference on Computer Vision (ICCV), 618–26.\n","        Venice: IEEE, 2017. https://doi.org/10.1109/ICCV.2017.74.\n","\n","    Args:\n","        model (keras.Model)\n","        img_bg_data (List): List of image arrays (rank 3 numpy arrays)\n","        img_bg_label (nd.array): numpy array (rank 2) e.g. [[0 1][1 0][1 0]]\n","        layer_name_for_grad_cal (str): Ideally layer.name of last convolution/ pooling layer\n","\n","        dim (tuple): Gradcam will run through top (r x c) and output in a grid plot\n","    \"\"\"\n","\n","    grad_model = keras.Model(\n","        model.inputs, [model.get_layer(layer_name_for_grad_cal).output, model.output]\n","    )\n","\n","    #Initializing matplotlib parameters\n","    alpha = 0.4\n","    rows, cols = dim\n","    r_c, c_c = 0, 0\n","    fig = plt.figure(figsize=(24, 16))\n","\n","    #Looping over for i = rows x cols\n","    for i, img in tqdm(enumerate(img_bg_data[:(rows*cols)])):\n","        fig.add_subplot(rows, cols, i+1)\n","\n","        #Initialising train images for gradcam from img_bg_data\n","        img_array = np.expand_dims(img, axis=0)\n","\n","        #Here onwards ref: https://keras.io/examples/vision/grad_cam/\n","        #Track gradient change in grad_model with img_array\n","        with tf.GradientTape() as tape:\n","            last_conv_layer_output, preds = grad_model(img_array)\n","            pred_index = tf.argmax(preds, axis = -1)\n","            class_channel = preds[:,int(pred_index.numpy())]\n","\n","        # Initialising labels (segregating for 1 or 2 output models)\n","        if len(img_bg_label.shape) != 1:\n","            model_pred = pred_index.numpy()\n","            grd_truth = np.argmax(img_bg_label,axis=1)[i]\n","        else:\n","            model_pred = int(tf.cast(preds>0.5, tf.int32).numpy())\n","            grd_truth = img_bg_label[i]\n","\n","        # Gradient of output with respect to last convolution layer\n","        grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","        # This is a vector where each entry is the mean intensity of the gradient\n","        # over a specific feature map channel\n","        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","        # We multiply each channel in the feature map array\n","        # by \"how important this channel is\" with regard to the top predicted class\n","        # then sum all the channels to obtain the heatmap class activation\n","        last_conv_layer_output = last_conv_layer_output[0]\n","        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","\n","        heatmap = tf.squeeze(heatmap)\n","        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","        heatmap = np.uint8(255 * heatmap)\n","\n","        # Use jet colormap to colorize heatmap\n","        jet = mpl.colormaps[\"jet\"]\n","        jet_colors = jet(np.arange(256))[:, :3]\n","        jet_heatmap = jet_colors[heatmap]\n","\n","        # Create an image with RGB colorized heatmap\n","        jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","        jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n","        jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","        # Superimpose the heatmap on original image\n","        superimposed_img = jet_heatmap * alpha + img\n","        superimposed_img = keras.utils.array_to_img(superimposed_img)\n","\n","        # Display Grad CAM\n","        plt.imshow(superimposed_img)\n","\n","        if int(model_pred) != int(grd_truth):\n","            plt.title(f\"id:{i}; Pred {int(model_pred)}; Truth {int(grd_truth)}\", color=\"red\")\n","        else:\n","            plt.title(f\"id:{i}; Pred {int(model_pred)}; Truth {int(grd_truth)}\")\n","\n","        plt.tight_layout()\n","        plt.axis(False)\n","\n","        r_c+=1\n","        c_c+=1\n","\n","    plt.subplots_adjust(wspace=0)\n","    plt.show()\n","    return"]},{"cell_type":"markdown","metadata":{"id":"d90-731TLNQu"},"source":["# <a id = \"prepare\"></a> <span style = \"font-family: Cambria\">**Data Preparation**</span>\n","This part consists of:\n","1. Loading and preparing unique images\n","2. Splitting dataset into training and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qy1tU-mPLNQu","outputId":"5563989c-f8f0-4e12-ee6b-acfc7a9dbea8"},"outputs":[],"source":["# load data\n","b_dataset = Path(\"./Dataset/Image Dataset/Brain Tumor/data/benign\")\n","m_dataset = Path(\"./Dataset/Image Dataset/Brain Tumor/data/malignant\")\n","\n","b_dataList = fileList(b_dataset)\n","m_dataList = fileList(m_dataset)\n","\n","#Excluding outlier non-brain MRI image\n","outlier = \"Dataset\\\\Image Dataset\\\\Brain Tumor\\\\data\\\\malignant\\\\248.jpg\"\n","if outlier in m_dataList:\n","    print(\"Removing outlier\")\n","    plt.imshow(cv2.imread(outlier))\n","    plt.show()\n","    m_dataList.remove(outlier)\n","\n","# preprocess images\n","b_imgs = preprocessing(b_dataList)\n","m_imgs = preprocessing(m_dataList)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7uNyuPz5LNQu","outputId":"ed99d4e8-3e3c-4794-e9fa-3d2d85366b9a"},"outputs":[],"source":["# Visualise pre-processed images\n","grid_plot(b_imgs)\n","grid_plot(m_imgs)"]},{"cell_type":"markdown","metadata":{"id":"n9543MBJLNQu"},"source":["### <span style = \"font-family: Cambria\">Split data into training and testing data</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AdFL9Gx_LNQx","outputId":"4e1ae062-3bfb-401c-d4e5-63b0156d47ef"},"outputs":[],"source":["# Initialising labels\n","b_label = [0 for _ in range(len(b_imgs))]\n","m_label = [1 for _ in range(len(m_imgs))]\n","\n","# Mixing order of class 0 and class 1\n","t_imgs = b_imgs + m_imgs\n","t_label = b_label + m_label\n","t_List = list(zip(t_imgs, t_label))\n","\n","t_imgs, t_label = map(list,(zip(*t_List)))\n","X, test_X, y, test_y = train_test_split(t_imgs, t_label, train_size=0.8, shuffle=True, random_state=42)\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","print(f\"Train: {len(y)} Test: {len(test_y)}\")"]},{"cell_type":"markdown","metadata":{"id":"SKdAOUgHLNQx"},"source":["# <a id = \"baseline\"></a> <span style = \"font-family: Cambria\">**Baseline model**</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srOQJ9B9LNQx"},"outputs":[],"source":["# Additional variable changes to adapt to baseline model\n","y_base = y.astype(np.float32)\n","test_X_base = np.array(test_X)\n","test_y_base = np.array(test_y).astype(np.float32)\n","img_width, img_height = 224, 224"]},{"cell_type":"markdown","metadata":{"id":"1FkA8h53LNQx"},"source":["## <span style = \"font-family: Cambria\">Baseline model creation</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_Qh2TSOLNQx"},"outputs":[],"source":["# Evaluation metrics\n","METRICS = [\"AUC\", \"binary_accuracy\", \"TruePositives\", \"TrueNegatives\", \"FalsePositives\", \"FalseNegatives\", \"Precision\", \"Recall\", \"F1Score\"]\n","\n","# A function to create baseline model\n","def create_baseline_model(learning_rate = 0.001, optimizer = SGD, num_layers = 2, dropout_rate = 0.4):\n","    '''\n","    Create baseline model for hyperparameter tuning or classification.\n","\n","    Args:\n","        learning_rate (float): The learning rate in model compile.\n","        optimizer (keras.optimizer): Optimizer of choice.\n","        num_layers (int): Number of convolutional layers expanded after the first 32-filter Conv layer\n","        dropout_rate (float): The probability of dropping out each unit in the hidden layer to prevent overfitting.\n","\n","    Returns:\n","        baseline_model: The model created with default / best hyperparameters.\n","    '''\n","\n","    baseline_model = Sequential()\n","    baseline_model.add(Input(shape = (img_width, img_height, 3)))\n","\n","    # A single 32-filter Conv layer to extract low-level features i.e., contours\n","    baseline_model.add(Conv2D(filters=32, kernel_size=3, activation='relu', padding='valid'))\n","    baseline_model.add(BatchNormalization()) # to normalise ReLU activation function to speed up and stabilise training\n","    baseline_model.add(MaxPool2D(pool_size=(2, 2))) # Condense feature map\n","\n","    # Expand Conv layers to capture increasingly complex features\n","    for _ in range(num_layers):\n","        baseline_model.add(Conv2D(64 * (_ + 1), kernel_size=3, activation='relu', padding = \"valid\"))\n","        baseline_model.add(MaxPool2D((2, 2))) # Condense feature map\n","\n","    # Flatten 3D output into 1D array before feeding into a fully connected layer\n","    baseline_model.add(Flatten())\n","    baseline_model.add(Dense(256, activation='relu')) # Extract features\n","    baseline_model.add(Dropout(dropout_rate)) # prevent overfitting\n","    baseline_model.add(Dense(1, activation='sigmoid')) # sigmoid fn to compute probability that the image belongs to malignant class\n","\n","    # optimizer used to adjust weights and learning rates by minimising loss\n","    if optimizer == 'adam':\n","        opt = Adam(learning_rate=learning_rate)\n","    else:\n","        opt = SGD(learning_rate=learning_rate)\n","\n","    # compile model against binary cross entropy LF and evaluation metrics\n","    baseline_model.compile(optimizer= opt, loss=keras.losses.binary_crossentropy, metrics=METRICS)\n","\n","    return baseline_model\n","\n","create_baseline_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"udbKz_lILNQx"},"source":["## <span style = \"font-family: Cambria\">Finetuning hyperparameters</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bi77VOIULNQx","outputId":"869bf668-c597-4f7f-9d37-9fbfa4ebf23f"},"outputs":[],"source":["# To solve kernel death when running GridSearchCV\n","os.environ['KMP_DUPLICATE_LIB_OK'] = \"True\"\n","\n","# Initialise the combinations of hyperparameter\n","param_grid = {\n","    'model__learning_rate': [0.0001, 0.001, 0.01],\n","    'model__optimizer': ['adam', 'sgd'],\n","    'model__num_layers': [2, 3, 4],\n","    'model__dropout_rate': [0.3, 0.4, 0.5],\n","    'batch_size': [32, 64, 128]\n","}\n","\n","keras_wrapper = KerasClassifier(build_fn=create_baseline_model, verbose = 0)\n","\n","# StratifiedKFold CV to maintain class distribution\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","grid_search = GridSearchCV(estimator=keras_wrapper, param_grid=param_grid, cv=skf, scoring='roc_auc', n_jobs= 6)\n","\n","# Upsample minority class\n","X, y = augment_minority_class(X, y)\n","\n","# Augment images to make model more robust\n","X = augment_images(X)\n","\n","grid_search.fit(X, y)\n","\n","# find best parameters\n","print(\"Best hyperparameters:\", grid_search.best_params_)\n","print(\"Best score:\", grid_search.best_score_)\n","\n","best_model = grid_search.best_estimator_\n","test_score = best_model.score(test_X, test_y)\n","print(\"Test AUC:\", test_score) # the score in this output is low, but this run may be an artefact as evident in the next cell"]},{"cell_type":"markdown","metadata":{"id":"pJbybe_KLNQx"},"source":["### <span style = \"font-family: Cambria\">Feeding best hyperparameters from GridSearchCV to get fine-tuned model</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFjDhbp6LNQy","outputId":"3a259bab-21cd-48a4-845d-ac8f6e005cff"},"outputs":[],"source":["# hyperparameters\n","num_folds = 5\n","epochs = 10\n","batch_size = 64\n","best_val_loss = 1000\n","filepath=\"baseline_model.weights.h5\"\n","\n","# from GridSearchCV\n","dropout_rate = 0.4\n","learning_rate = 0.01\n","num_layers = 3\n","optimizer = SGD\n","\n","skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n","\n","scores = {}\n","for fold_idx, (train_idx, val_idx) in tqdm(enumerate(skf.split(X, y))):\n","\n","    # Reinitialising model\n","    train_X, val_X = X[train_idx], X[val_idx]\n","    train_y, val_y = y[train_idx], y[val_idx]\n","\n","    # augment the minority (benign) class\n","    train_X, train_y = augment_minority_class(train_X, train_y)\n","\n","    # increase dataset and standardise trainig data\n","    train_generator = train_datagen.flow(\n","            x=train_X,\n","            y=train_y,\n","            batch_size=batch_size)\n","\n","    # standardise validation data\n","    val_generator = validation_datagen.flow(\n","        x=val_X,\n","        y=val_y,\n","        batch_size = batch_size\n","    )\n","\n","    # clear previous iteration and reinitialise model\n","    keras.backend.clear_session()\n","    baseline_model = create_baseline_model(num_layers=num_layers, dropout_rate=dropout_rate, learning_rate=learning_rate, optimizer=optimizer)\n","\n","    baseline_history = baseline_model.fit(train_generator,\n","                        epochs=epochs,\n","                        validation_data = val_generator)\n","\n","    if baseline_history.history[\"val_loss\"][-1] <= best_val_loss:\n","        best_val_loss = baseline_history.history[\"val_loss\"][-1] #replace\n","        baseline_model.save_weights(filepath)\n","\n","    scores[\"loss\"], scores[\"AUC\"], scores[\"binary_accuracy\"], scores[\"TP\"], scores[\"TN\"], scores[\"FP\"], scores[\"FN\"], scores[\"precision\"], scores[\"recall\"],  scores[\"f1_score\"] = baseline_model.evaluate(val_generator)\n","\n","    print(f\"Fold {fold_idx + 1}: {scores}\")\n","\n","# load best weights\n","baseline_model = create_baseline_model(num_layers=num_layers, dropout_rate=dropout_rate, learning_rate=learning_rate, optimizer=optimizer)\n","baseline_model.load_weights(filepath)\n","\n","# preprocessing test data\n","test_generator = validation_datagen.flow(\n","        x=test_X,\n","        y=test_y\n",")\n","\n","scores[\"loss\"], scores[\"AUC\"], scores[\"binary_accuracy\"], scores[\"TP\"], scores[\"TN\"], scores[\"FP\"], scores[\"FN\"], scores[\"precision\"], scores[\"recall\"],  scores[\"f1_score\"] = baseline_model.evaluate(test_generator)\n","print(f\"Performance metrics: {scores}\")"]},{"cell_type":"markdown","metadata":{"id":"RZx9pnjHLNQy"},"source":["### <span style = \"font-family: Cambria\">Plot AUC and loss graphs</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaubugeSLNQy","outputId":"2db5025a-8cb8-4d7f-ceb3-f152215f595e"},"outputs":[],"source":["\n","# Plot training & validation AUC values\n","plt.figure(figsize=(10, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(baseline_history.history['auc'], color = \"midnightblue\")\n","plt.plot(baseline_history.history['val_auc'], color = \"salmon\")\n","plt.ylabel('AUC of Baseline CNN')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper right')\n","\n","# Plot training & validation loss values\n","plt.subplot(1, 2, 2)\n","plt.plot(baseline_history.history['loss'], color = \"midnightblue\")\n","plt.plot(baseline_history.history['val_loss'], color = \"salmon\")\n","plt.ylabel('Loss of Baseline CNN')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='center right')\n","\n","plt.tight_layout()\n","plt.savefig(\"model_metrics.jpg\", dpi = 250)\n"]},{"cell_type":"markdown","metadata":{"id":"ebdkaY6ALNQy"},"source":["### <span style = \"font-family: Cambria\">Gradcam</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGnCrXTSLNQy","outputId":"3e15a2c3-7496-4307-9d78-97e8f73b922a"},"outputs":[],"source":["grad_heatmap(baseline_model, X_aug, y_aug, \"conv2d_5\", dim=(7,10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMsc9eIbLNQy","outputId":"761a7089-8253-4f1b-b757-a23068781949"},"outputs":[],"source":["grad_heatmap(baseline_model, test_X_base, test_y_base, \"conv2d_5\", dim=(4,10))"]},{"cell_type":"markdown","metadata":{"id":"BEGEu6oCLNQy"},"source":["# <a id = \"ann\"></a><span style = \"font-family: Cambria\">**VGG16-ANN model**</span>"]},{"cell_type":"markdown","metadata":{"id":"cdjp2CArLNQy"},"source":["### <span style = \"font-family: Cambria\">Transfer learning experiment</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3xk9K16LNQy","outputId":"e87d1cc7-bb73-4269-9745-fc74f5086194"},"outputs":[],"source":["# Will be looping through this to unfreeze layers from the back for training\n","# during finetuning\n","\n","layers_frozen = [2, 3, 8, 9, 10, 12, 13, 14, 16, 17]\n","\n","\"\"\"\n","Note: Runtime for this cell is very long\n","\n","Aim: To understand the effects of fine-tuning from different convolution layers of VGG16,\n","we first trained the custom classifier for 10 epochs, before freezing the first n layers\n","and training the rest of the convolutional layers at a lower learning rate.\n","\n","Algorithm flow:\n","\n","Outer loop around StratifiedKFold(n_split = 5): {\n","\n","    Train base VGG16-ANN model while only training custom classifier at back of model\n","    Save base model weights\n","\n","    Inner loop around layers_frozen: {\n","        Load base model weights\n","        Run through layers_frozen to unfreeze selected layers\n","        Finetune model for all the unfrozen layers\n","        Save new model weights\n","    }\n","\n","}\n","\n","Refer to the next segment \"Transfer learning exp summary\" for summary of results\n","\"\"\"\n","batch_size = 64\n","epochs = 10\n","epochs_finetune = 10\n","\n","finetune_means = []\n","finetune_errors = []\n","\n","best_val_loss = 10000\n","\n","# To record performance across layers_frozen loops\n","layer_metrics = {l:[] for l in layers_frozen}\n","\n","\n","# Running experiment across stratifiedkfold\n","skf = StratifiedKFold(n_splits=5)\n","for fold_idx, (train_idx, val_idx) in tqdm(enumerate(skf.split(X, y))):\n","\n","    fold_filepath = f\"./tf_exp/vggbasemodel_{fold_idx}.h5\"\n","    model = create_vgg16tf()\n","\n","    # Segregating train and val\n","    train_X, val_X = X[train_idx], X[val_idx]\n","    train_y_old, val_y_old = y[train_idx], y[val_idx]\n","\n","    train_X, train_y_old = augment_minority_class(train_X, train_y_old)\n","    train_y = np.array([[1,0] if l == 0 else [0,1] for l in train_y_old])\n","    val_y = np.array([[1,0] if l == 0 else [0,1] for l in val_y_old])\n","\n","    # To flow train and val data into model\n","    train_generator = train_datagen.flow(\n","        x=train_X,\n","        y=train_y,\n","        batch_size=batch_size)\n","\n","    validation_generator = validation_datagen.flow(\n","        x=val_X,\n","        y=val_y,\n","        shuffle=False,\n","        batch_size=batch_size)\n","\n","    model.compile(optimizer=Adam(learning_rate=0.0001),\n","            loss=\"categorical_crossentropy\",\n","            metrics=[\"binary_accuracy\", \"AUC\"])\n","\n","    epoch_history = model.fit(\n","        train_generator,\n","        epochs=epochs,\n","        validation_data=validation_generator,\n","        verbose=1)\n","\n","    # Tracking metrics for base model training\n","    fold_trainloss = epoch_history.history[\"loss\"]\n","    fold_trainacc = epoch_history.history[\"binary_accuracy\"]\n","    fold_valloss = epoch_history.history[\"val_loss\"]\n","    fold_valacc = epoch_history.history[\"val_binary_accuracy\"]\n","\n","    # Saving model for use later\n","    model.save_weights(fold_filepath)\n","\n","\n","    # Finetuning loop\n","    for layer_num in layers_frozen:\n","\n","        # New file path for finetune model\n","        layer_filepath = f\"./tf_exp/vggbasemodel_{fold_idx}_finetune_{layer_num}.h5\"\n","\n","        # Copy base model metrics to continue plot\n","        finetune_trainloss = fold_trainloss.copy()\n","        finetune_trainacc = fold_trainacc.copy()\n","        finetune_valloss = fold_valloss.copy()\n","        finetune_valacc = fold_valacc.copy()\n","\n","        # Load base model for certain fold\n","        model = create_vgg16tf()\n","        model.load_weights(fold_filepath)\n","\n","        # Freezing weights\n","        for id, layer in enumerate(model.layers):\n","            layer.trainable = False\n","            if id > layer_num:\n","                layer.trainable = True\n","\n","        model.compile(optimizer=Adam(learning_rate=1e-4),\n","            loss=\"categorical_crossentropy\",\n","            metrics=[\"binary_accuracy\", \"AUC\"])\n","\n","        epoch_history_ft = model.fit(\n","            train_generator,\n","            epochs=epochs_finetune,\n","            validation_data=validation_generator,\n","            verbose=1)\n","        model.save_weights(layer_filepath)\n","\n","        # Continue plot with finetune metrics\n","        finetune_trainloss.extend(epoch_history_ft.history[\"loss\"])\n","        finetune_trainacc.extend(epoch_history_ft.history[\"binary_accuracy\"])\n","        finetune_valloss.extend(epoch_history_ft.history[\"val_loss\"])\n","        finetune_valacc.extend(epoch_history_ft.history[\"val_binary_accuracy\"])\n","\n","        #Pickle dump training history\n","        finetune_history = {\n","            \"loss\": finetune_trainloss,\n","            \"acc\": finetune_trainacc,\n","            \"val_loss\": finetune_valloss,\n","            \"val_acc\": finetune_valacc\n","        }\n","\n","        # Dumping finetune_history for sanity measures\n","        pickle.dump(finetune_history, open(f\"./tf_exp/fold_{fold_idx}_finetune_{layer_num}.pkl\", \"wb\"))\n","\n","        loss, accuracy, auc = model.evaluate(validation_generator)\n","        preds = model.predict(validation_generator)\n","        buffer = [loss]\n","        buffer.extend(added_metrics(preds, val_y))\n","\n","        layer_metrics[layer_num].append(buffer)\n","\n","        print(f\"Fold{fold_idx} Layer{layer_num}: {added_metrics(preds, val_y)}\")\n","\n","        # History.history plots\n","        fig, ax = plt.subplots(1,2,figsize=(13,6))\n","\n","        ax[0].plot(finetune_trainacc, label=\"train_acc\")\n","        ax[0].plot(finetune_valacc, label=\"val_acc\")\n","        ax[0].axvline(x=epochs, ymin =0, ymax=10, color=\"red\", alpha=0.5, linestyle=\"--\")\n","        ax[0].set_xlabel(\"Epoch\")\n","        ax[0].set_title(f\"Fold {fold_idx+1}\")\n","        ax[0].legend()\n","\n","        ax[1].plot(finetune_trainloss, label=\"train_loss\")\n","        ax[1].plot(finetune_valloss, label=\"val_loss\")\n","        ax[1].axvline(x=epochs, ymin =0, ymax=10, color=\"red\", alpha=0.5, linestyle=\"--\")\n","        ax[1].set_xlabel(\"Epoch\")\n","        ax[1].legend()\n","        plt.show()\n","\n","        keras.backend.clear_session()"]},{"cell_type":"markdown","metadata":{"id":"eUH4VH6nLNQy"},"source":["#### Transfer learning exp summary"]},{"cell_type":"markdown","metadata":{"id":"Zw4_ZeaPLNQy"},"source":["##### Val predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofeHG3VeLNQz","outputId":"62afba10-c6cc-4cf9-b093-7c20af12a2ca"},"outputs":[],"source":["layers_frozen = [2, 3, 8, 9, 10, 12, 13, 14, 16, 17]\n","layers_frozen = [i + 2 for i in layers_frozen]              # Need to add 2 due to calculations (index start from 0 and id>layer_num)\n","\n","layer_means = []\n","layer_errors = []\n","for layer_num, fold_metrics in layer_metrics.items():\n","    fold_metrics = np.array(fold_metrics)\n","    fold_means = np.mean(fold_metrics, axis = 0)\n","    fold_errors = np.std(fold_metrics, axis = 0)\n","\n","    layer_means.append(fold_means)\n","    layer_errors.append(fold_errors)\n","\n","means = list(map(list,(zip(*layer_means))))\n","errors = list(map(list,(zip(*layer_errors))))\n","\n","print(\"\\nMeans\\n\")\n","print(means)\n","print(\"\\nErrors\\n\")\n","print(errors)\n","print(\"\\n\")\n","\n","fig, ax = plt.subplots(1,3,figsize=(13,6))\n","\n","ax[0].errorbar(layers_frozen, means[0], yerr=errors[0], label=\"val_loss\")\n","ax[0].set_xlabel(\"Layers frozen\")\n","ax[0].legend()\n","\n","ax[1].errorbar(layers_frozen, means[1], yerr=errors[1], label=\"TN\")\n","ax[1].errorbar(layers_frozen, means[2], yerr=errors[2], label=\"TP\")\n","ax[1].errorbar(layers_frozen, means[3], yerr=errors[3], label=\"FN\")\n","ax[1].errorbar(layers_frozen, means[4], yerr=errors[4], label=\"FP\")\n","ax[1].set_xlabel(\"Layers frozen\")\n","ax[1].legend()\n","\n","ax[2].errorbar(layers_frozen, means[5], yerr=errors[5], label=\"precision\")\n","ax[2].errorbar(layers_frozen, means[6], yerr=errors[6], label=\"accuracy\")\n","ax[2].errorbar(layers_frozen, means[7], yerr=errors[7], label=\"recall\")\n","ax[2].set_xlabel(\"Layers frozen\")\n","ax[2].legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyexwvkOLNQz","outputId":"43565d2f-861d-452d-dafc-21076ae2d743"},"outputs":[],"source":["# Copied the means and SD as the code block for the experiment took 2-3hrs to run\n","# Just in case it went missing, and to facilitate analysis after the session\n","\n","layers_frozen = [2, 3, 8, 9, 10, 12, 13, 14, 16, 17]\n","layers_frozen = [i + 2 for i in layers_frozen]              # Need to add 2 due to calculations (index start from 0 and id>layer_num)\n","\n","means = [[0.6460669994354248, 0.591467696428299, 0.6758981585502625, 0.5428489506244659, 0.6624845623970032, 0.4607712388038635, 0.4052444934844971, 0.5576117932796478, 0.4481421530246735, 0.39641469717025757], [6.4, 4.8, 4.8, 5.2, 8.8, 6.4, 8.2, 7.4, 6.6, 8.4], [12.2, 16.8, 14.6, 17.2, 10.2, 18.4, 18.6, 18.4, 19.0, 17.6], [4.0, 5.6, 5.6, 5.2, 1.6, 4.0, 2.2, 3.0, 3.8, 2.0], [8.0, 3.4, 5.6, 3.0, 10.0, 1.8, 1.6, 1.8, 1.2, 2.6], [0.6038095238095238, 0.8357142857142857, 0.7219047619047618, 0.8538095238095238, 0.5023809523809524, 0.9104761904761904, 0.9204761904761904, 0.9114285714285714, 0.9404761904761905, 0.8709523809523809], [0.6086021505376344, 0.7075268817204301, 0.6337634408602151, 0.7335483870967743, 0.6206451612903225, 0.8111827956989247, 0.8754838709677418, 0.843010752688172, 0.8369892473118281, 0.8494623655913978], [0.7700314889788574, 0.7627224627224628, 0.7711167182267795, 0.7853002070393376, 0.7214141414141414, 0.824503105590062, 0.8981254189492176, 0.8631032125768968, 0.8405035865905433, 0.9051171406777815]]\n","errors = [[0.05789509557340848, 0.14546233125374053, 0.09779906780071315, 0.14980663076625234, 0.15593145096142064, 0.18000572987292435, 0.1252150276740922, 0.19599881757688642, 0.12262273448221743, 0.04791983317594967], [2.4166091947189146, 2.4819347291981715, 3.249615361854384, 2.6381811916545836, 1.1661903789690602, 1.4966629547095764, 1.3266499161421599, 1.3564659966250536, 1.8547236990991407, 1.3564659966250536], [4.707440918375928, 4.069397989875161, 4.673328578219169, 2.481934729198171, 6.4, 1.2000000000000002, 1.0198039027185568, 1.019803902718557, 0.6324555320336759, 1.4966629547095764], [2.449489742783178, 2.939387691339814, 3.6110940170535577, 3.059411708155671, 1.3564659966250536, 1.6733200530681511, 1.16619037896906, 1.0954451150103321, 2.2271057451320084, 1.2649110640673518], [4.69041575982343, 4.454211490264017, 4.586937976471886, 2.8284271247461903, 6.2289646009589745, 0.9797958971132713, 0.8, 1.16619037896906, 0.4, 1.3564659966250536], [0.2346445181260822, 0.2116986691753452, 0.23004189700230093, 0.13473413814568083, 0.3133897763607448, 0.04938625585700636, 0.04036679895718616, 0.056892307862113946, 0.02025909274874535, 0.06826983942457085], [0.08846267338933866, 0.12368115684377644, 0.05821406468250258, 0.10061298998733334, 0.17192791849239608, 0.07355530370546157, 0.026130580953262834, 0.014183769847605276, 0.06687307623148353, 0.017668469596940826], [0.05549313187578253, 0.0789901210078336, 0.12456858000894624, 0.10278538753270194, 0.36533631826904783, 0.06429224519490624, 0.04705231632623448, 0.035720820157888085, 0.0744328107363998, 0.05546636624229973]]\n","\n","\n","fig, ax = plt.subplots(1,2,figsize=(13,6))\n","\n","ax[0].plot(layers_frozen, means[0], label=\"val_loss\", color=\"midnightblue\")\n","ax[0].axvline(x = 11, ymin = 0, ymax=30, color=\"midnightblue\", alpha = 0.3, linestyle=\"--\")\n","ax[0].axvline(x = 15, ymin = 0, ymax=30, color=\"midnightblue\", alpha = 0.3, linestyle=\"--\")\n","ax[0].set_xlabel(\"Layers frozen\")\n","ax[0].set_ylabel(\"Validation loss\")\n","ax[0].legend()\n","\n","ax[1].plot(layers_frozen, means[5], label=\"precision\", color=\"midnightblue\")\n","ax[1].plot(layers_frozen, means[6], label=\"accuracy\", color=\"salmon\")\n","ax[1].plot(layers_frozen, means[7], label=\"recall\", color=\"lightpink\")\n","ax[1].axvline(x = 11, ymin = 0, ymax=30, color=\"midnightblue\", alpha = 0.3, linestyle=\"--\")\n","ax[1].axvline(x = 15, ymin = 0, ymax=30, color=\"midnightblue\", alpha = 0.3, linestyle=\"--\")\n","ax[1].set_xlabel(\"Layers frozen\")\n","ax[1].set_ylabel(\"Metrics score\")\n","ax[1].legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4BQonWRLNQz","outputId":"c04c0975-edf3-480d-e4ba-fc42f9409945"},"outputs":[],"source":["val_tfexp_means = [[0.6460669994354248, 0.591467696428299, 0.6758981585502625, 0.5428489506244659, 0.6624845623970032, 0.4607712388038635, 0.4052444934844971, 0.5576117932796478, 0.4481421530246735, 0.39641469717025757], [6.4, 4.8, 4.8, 5.2, 8.8, 6.4, 8.2, 7.4, 6.6, 8.4], [12.2, 16.8, 14.6, 17.2, 10.2, 18.4, 18.6, 18.4, 19.0, 17.6], [4.0, 5.6, 5.6, 5.2, 1.6, 4.0, 2.2, 3.0, 3.8, 2.0], [8.0, 3.4, 5.6, 3.0, 10.0, 1.8, 1.6, 1.8, 1.2, 2.6], [0.6038095238095238, 0.8357142857142857, 0.7219047619047618, 0.8538095238095238, 0.5023809523809524, 0.9104761904761904, 0.9204761904761904, 0.9114285714285714, 0.9404761904761905, 0.8709523809523809], [0.6086021505376344, 0.7075268817204301, 0.6337634408602151, 0.7335483870967743, 0.6206451612903225, 0.8111827956989247, 0.8754838709677418, 0.843010752688172, 0.8369892473118281, 0.8494623655913978], [0.7700314889788574, 0.7627224627224628, 0.7711167182267795, 0.7853002070393376, 0.7214141414141414, 0.824503105590062, 0.8981254189492176, 0.8631032125768968, 0.8405035865905433, 0.9051171406777815]]\n","val_tfexp_errors = [[0.05789509557340848, 0.14546233125374053, 0.09779906780071315, 0.14980663076625234, 0.15593145096142064, 0.18000572987292435, 0.1252150276740922, 0.19599881757688642, 0.12262273448221743, 0.04791983317594967], [2.4166091947189146, 2.4819347291981715, 3.249615361854384, 2.6381811916545836, 1.1661903789690602, 1.4966629547095764, 1.3266499161421599, 1.3564659966250536, 1.8547236990991407, 1.3564659966250536], [4.707440918375928, 4.069397989875161, 4.673328578219169, 2.481934729198171, 6.4, 1.2000000000000002, 1.0198039027185568, 1.019803902718557, 0.6324555320336759, 1.4966629547095764], [2.449489742783178, 2.939387691339814, 3.6110940170535577, 3.059411708155671, 1.3564659966250536, 1.6733200530681511, 1.16619037896906, 1.0954451150103321, 2.2271057451320084, 1.2649110640673518], [4.69041575982343, 4.454211490264017, 4.586937976471886, 2.8284271247461903, 6.2289646009589745, 0.9797958971132713, 0.8, 1.16619037896906, 0.4, 1.3564659966250536], [0.2346445181260822, 0.2116986691753452, 0.23004189700230093, 0.13473413814568083, 0.3133897763607448, 0.04938625585700636, 0.04036679895718616, 0.056892307862113946, 0.02025909274874535, 0.06826983942457085], [0.08846267338933866, 0.12368115684377644, 0.05821406468250258, 0.10061298998733334, 0.17192791849239608, 0.07355530370546157, 0.026130580953262834, 0.014183769847605276, 0.06687307623148353, 0.017668469596940826], [0.05549313187578253, 0.0789901210078336, 0.12456858000894624, 0.10278538753270194, 0.36533631826904783, 0.06429224519490624, 0.04705231632623448, 0.035720820157888085, 0.0744328107363998, 0.05546636624229973]]\n","\n","layers_frozen = [2, 3, 8, 9, 10, 12, 13, 14, 16, 17]\n","layers_frozen = [i + 2 for i in layers_frozen]              # Need to add 2 due to calculations (index start from 0 and id>layer_num)\n","\n","val_tf_exp = pd.DataFrame(np.array(val_tfexp_means).T.tolist(), columns = [\"Val_loss\", \"TN\", \"TP\", \"FN\", \"FP\", \"Precision\", \"Accuracy\", \"Recall\"])\n","val_tf_exp[\"Layers frozen\"] = layers_frozen\n","val_tf_exp = val_tf_exp.set_index(\"Layers frozen\")\n","val_tf_exp\n"]},{"cell_type":"markdown","metadata":{"id":"iw8HjtI6LNQz"},"source":["##### Test predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDkQQP-DLNQz","outputId":"88b9486b-e72d-40ba-e70b-ccaebbc08547"},"outputs":[],"source":["layers_frozen = [2, 3, 8, 9, 10, 12, 13, 14, 16, 17]\n","batch_size = 64\n","\n","test_X_new = np.array(test_X)\n","test_y_new = np.array([[1,0] if l == 0 else [0,1] for l in test_y])\n","\n","test_datagen = ImageDataGenerator(\n","    samplewise_center = True,\n","    samplewise_std_normalization = True)\n","\n","test_generator = test_datagen.flow(\n","    x=test_X_new,\n","    y=test_y_new,\n","    shuffle=False,\n","    batch_size=batch_size)\n","\n","layer_ft_mean = []\n","layer_ft_error = []\n","\n","for layer_num in layers_frozen:\n","    fold_buffer = []\n","    for fold_idx in range(5):\n","        model = create_vgg16tf()\n","        model.load_weights(f\"./tf_exp/vggbasemodel_{fold_idx}_finetune_{layer_num}.h5\")\n","\n","        model.compile(optimizer=Adam(learning_rate=1e-4),\n","            loss=\"categorical_crossentropy\",\n","            metrics=[\"binary_accuracy\", \"AUC\"])\n","\n","        preds = model.predict(test_generator)\n","        fold_buffer.append(added_metrics(preds, test_y_new))\n","    layer_ft_mean.append(np.mean(fold_buffer, axis = 0))\n","    layer_ft_error.append(np.std(fold_buffer, axis = 0))\n","\n","means = list(map(list,(zip(*layer_ft_mean))))\n","errors = list(map(list,(zip(*layer_ft_error))))\n","\n","print(\"\\nMeans\\n\")\n","print(means)\n","print(\"\\nErrors\\n\")\n","print(errors)\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEsgn8hrLNQz","outputId":"27f92efa-7700-4a8b-dbc3-730398f312e9"},"outputs":[],"source":["means = [[6.2, 4.0, 4.4, 5.4, 7.4, 4.8, 5.4, 5.8, 5.0, 5.4], [16.8, 25.8, 23.0, 22.6, 15.4, 24.6, 24.8, 26.2, 24.4, 22.6], [3.8, 6.0, 5.6, 4.6, 2.6, 5.2, 4.6, 4.2, 5.0, 4.6], [12.2, 3.2, 6.0, 6.4, 13.6, 4.4, 4.2, 2.8, 4.6, 6.4], [0.5793103448275863, 0.8896551724137932, 0.793103448275862, 0.7793103448275862, 0.5310344827586206, 0.8482758620689657, 0.8551724137931034, 0.903448275862069, 0.8413793103448276, 0.7793103448275861], [0.5897435897435898, 0.764102564102564, 0.7025641025641025, 0.7179487179487178, 0.5846153846153846, 0.7538461538461539, 0.7743589743589743, 0.8205128205128206, 0.7538461538461538, 0.7179487179487178], [0.8086697722567289, 0.8134637460040686, 0.8316794091641961, 0.8339805351848962, 0.8360027472527471, 0.8231996348931834, 0.8437950999686261, 0.8621506734006734, 0.8292578541118585, 0.8312282149523528]]\n","errors = [[1.6, 1.4142135623730951, 3.2619012860600183, 1.4966629547095764, 2.0591260281974, 0.7483314773547882, 0.48989794855663565, 0.7483314773547882, 0.0, 0.48989794855663565], [7.626270385975047, 3.1874754901018454, 5.89915248150105, 4.498888751680797, 8.138795979750322, 3.0724582991474434, 1.4696938456699067, 1.7204650534085253, 1.854723699099141, 1.2], [1.6, 1.4142135623730951, 3.2619012860600183, 1.4966629547095764, 2.0591260281974, 0.7483314773547882, 0.48989794855663565, 0.7483314773547882, 0.0, 0.48989794855663565], [7.626270385975047, 3.1874754901018454, 5.89915248150105, 4.498888751680797, 8.138795979750322, 3.0724582991474434, 1.4696938456699071, 1.7204650534085255, 1.8547236990991407, 1.2], [0.2629748408956913, 0.1099129479345464, 0.2034190510862431, 0.1551340948855447, 0.2806481372327697, 0.10594683790163598, 0.05067909812654853, 0.05932638115201811, 0.0639559896241083, 0.041379310344827586], [0.15554616295490362, 0.054754247446314415, 0.07360359022772989, 0.0959399329942036, 0.17571119691294165, 0.08970695222838923, 0.029902317409463055, 0.042905642386362845, 0.047557017925618984, 0.0229340305384594], [0.030103984222125443, 0.022015450511969507, 0.08832593675252555, 0.03969442682711525, 0.13566344923656123, 0.03469455269134492, 0.010006692278627872, 0.020417497382018646, 0.010696286385120024, 0.010914639976811802]]\n","\n","layers_frozen = [2, 3, 8, 9, 10, 12, 13, 14, 16, 17]\n","layers_frozen = [i + 2 for i in layers_frozen]              # Need to add 2 due to calculations (index start from 0 and id>layer_num)\n","\n","fig, ax = plt.subplots(1,2,figsize=(13,6))\n","\n","ax[0].errorbar(layers_frozen, means[0], label=\"TN\")\n","ax[0].errorbar(layers_frozen, means[1], label=\"TP\")\n","ax[0].errorbar(layers_frozen, means[2], label=\"FN\")\n","ax[0].errorbar(layers_frozen, means[3], label=\"FP\")\n","ax[0].axvline(x = 12, ymin = 0, ymax=30, color=\"red\", alpha = 0.3, linestyle=\"--\")\n","ax[0].axvline(x = 16, ymin = 0, ymax=30, color=\"red\", alpha = 0.3, linestyle=\"--\")\n","ax[0].axvline(x = 11, ymin = 0, ymax=30, color=\"green\", alpha = 0.3, linestyle=\"--\")\n","ax[0].axvline(x = 15, ymin = 0, ymax=30, color=\"green\", alpha = 0.3, linestyle=\"--\")\n","ax[0].axvline(x = 18, ymin = 0, ymax=30, color=\"green\", alpha = 0.3, linestyle=\"--\")\n","ax[0].set_xlabel(\"Layers frozen\")\n","ax[0].legend()\n","\n","ax[1].errorbar(layers_frozen, means[4], label=\"precision\")\n","ax[1].errorbar(layers_frozen, means[5], label=\"accuracy\")\n","ax[1].errorbar(layers_frozen, means[6], label=\"recall\")\n","ax[1].axvline(x = 12, ymin = 0, ymax=30, color=\"red\", alpha = 0.3, linestyle=\"--\")\n","ax[1].axvline(x = 16, ymin = 0, ymax=30, color=\"red\", alpha = 0.3, linestyle=\"--\")\n","ax[1].axvline(x = 11, ymin = 0, ymax=30, color=\"green\", alpha = 0.3, linestyle=\"--\")\n","ax[1].axvline(x = 15, ymin = 0, ymax=30, color=\"green\", alpha = 0.3, linestyle=\"--\")\n","ax[1].axvline(x = 18, ymin = 0, ymax=30, color=\"green\", alpha = 0.3, linestyle=\"--\")\n","ax[1].set_xlabel(\"Layers frozen\")\n","ax[1].legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxIArWzYLNQz","outputId":"151c6029-0573-40c6-febb-f66c6a6b986e"},"outputs":[],"source":["layers_frozen = [2, 3, 8, 9, 10, 12, 13, 14, 16, 17]\n","layers_frozen = [i + 2 for i in layers_frozen]              # Need to add 2 due to calculations (index start from 0 and id>layer_num)\n","\n","test_tfexp_means = [[6.2, 4.0, 4.4, 5.4, 7.4, 4.8, 5.4, 5.8, 5.0, 5.4], [16.8, 25.8, 23.0, 22.6, 15.4, 24.6, 24.8, 26.2, 24.4, 22.6], [3.8, 6.0, 5.6, 4.6, 2.6, 5.2, 4.6, 4.2, 5.0, 4.6], [12.2, 3.2, 6.0, 6.4, 13.6, 4.4, 4.2, 2.8, 4.6, 6.4], [0.5793103448275863, 0.8896551724137932, 0.793103448275862, 0.7793103448275862, 0.5310344827586206, 0.8482758620689657, 0.8551724137931034, 0.903448275862069, 0.8413793103448276, 0.7793103448275861], [0.5897435897435898, 0.764102564102564, 0.7025641025641025, 0.7179487179487178, 0.5846153846153846, 0.7538461538461539, 0.7743589743589743, 0.8205128205128206, 0.7538461538461538, 0.7179487179487178], [0.8086697722567289, 0.8134637460040686, 0.8316794091641961, 0.8339805351848962, 0.8360027472527471, 0.8231996348931834, 0.8437950999686261, 0.8621506734006734, 0.8292578541118585, 0.8312282149523528]]\n","test_tfexp_errors = [[1.6, 1.4142135623730951, 3.2619012860600183, 1.4966629547095764, 2.0591260281974, 0.7483314773547882, 0.48989794855663565, 0.7483314773547882, 0.0, 0.48989794855663565], [7.626270385975047, 3.1874754901018454, 5.89915248150105, 4.498888751680797, 8.138795979750322, 3.0724582991474434, 1.4696938456699067, 1.7204650534085253, 1.854723699099141, 1.2], [1.6, 1.4142135623730951, 3.2619012860600183, 1.4966629547095764, 2.0591260281974, 0.7483314773547882, 0.48989794855663565, 0.7483314773547882, 0.0, 0.48989794855663565], [7.626270385975047, 3.1874754901018454, 5.89915248150105, 4.498888751680797, 8.138795979750322, 3.0724582991474434, 1.4696938456699071, 1.7204650534085255, 1.8547236990991407, 1.2], [0.2629748408956913, 0.1099129479345464, 0.2034190510862431, 0.1551340948855447, 0.2806481372327697, 0.10594683790163598, 0.05067909812654853, 0.05932638115201811, 0.0639559896241083, 0.041379310344827586], [0.15554616295490362, 0.054754247446314415, 0.07360359022772989, 0.0959399329942036, 0.17571119691294165, 0.08970695222838923, 0.029902317409463055, 0.042905642386362845, 0.047557017925618984, 0.0229340305384594], [0.030103984222125443, 0.022015450511969507, 0.08832593675252555, 0.03969442682711525, 0.13566344923656123, 0.03469455269134492, 0.010006692278627872, 0.020417497382018646, 0.010696286385120024, 0.010914639976811802]]\n","\n","test_tf_exp = pd.DataFrame(np.array(test_tfexp_means).T.tolist(), columns = [\"TN\", \"TP\", \"FN\", \"FP\", \"Precision\", \"Accuracy\", \"Recall\"])\n","test_tf_exp[\"Layers frozen\"] = layers_frozen\n","test_tf_exp = test_tf_exp.set_index(\"Layers frozen\")\n","test_tf_exp\n"]},{"cell_type":"markdown","metadata":{"id":"XcQrDbP1LNQz"},"source":["## <span style = \"font-family: Cambria\">Final models</span>"]},{"cell_type":"markdown","metadata":{"id":"Hk3FPys4LNQz"},"source":["#### <span style = \"font-family: Cambria\">VGG16TF - All frozen</span>"]},{"cell_type":"markdown","metadata":{"id":"v69rtP5iLNQz"},"source":["##### Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IIb7AwfLNQz","outputId":"7c6a2e0f-7adf-4186-fc1b-8ce656cfe5c2"},"outputs":[],"source":["batch_size = 64\n","epochs = 15\n","\n","vgg16tf = create_vgg16tf()\n","\n","# Augmenting train data\n","X_aug, y_aug = augment_minority_class(X, y)\n","y_aug_new = np.array([[1,0] if l == 0 else [0,1] for l in y_aug])\n","\n","train_generator = train_datagen.flow(\n","    x=X_aug,\n","    y=y_aug_new,\n","    batch_size=batch_size)\n","\n","# Test data\n","test_X_new = np.array(test_X)\n","test_y_new = np.array([[1,0] if l == 0 else [0,1] for l in test_y])\n","\n","test_datagen = ImageDataGenerator(\n","    samplewise_center = True,\n","    samplewise_std_normalization = True)\n","\n","test_generator = test_datagen.flow(\n","    x=test_X_new,\n","    y=test_y_new,\n","    shuffle=False,\n","    batch_size=batch_size)\n","\n","optimizer = Adam(learning_rate=0.00005)\n","\n","vgg16tf.compile(optimizer=optimizer,\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"binary_accuracy\", \"AUC\"])\n","\n","early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, start_from_epoch=3)\n","\n","epoch_history = vgg16tf.fit(\n","    train_generator,\n","    epochs=epochs,\n","    validation_data=test_generator,\n","    callbacks=[early_stopping],\n","    verbose=1)\n","\n","\n","vggtf_trainloss = epoch_history.history[\"loss\"].copy()\n","vggtf_trainauc = epoch_history.history[\"auc\"].copy()\n","vggtf_valloss = epoch_history.history[\"val_loss\"].copy()\n","vggtf_valauc = epoch_history.history[\"val_auc\"].copy()\n","\n","\n","# Finetuned results\n","loss, accuracy, auc = vgg16tf.evaluate(test_generator)\n","preds = vgg16tf.predict(test_generator)\n","print(added_metrics(preds, test_y_new))\n","\n","# Saving weights of finetune model\n","vgg16tf.save_weights(f\"./tfexp/VGG16TF_ft.h5\")\n","\n","\n","# History.history plots\n","fig, ax = plt.subplots(1,2,figsize=(13,6))\n","\n","ax[0].plot(vggtf_trainauc, label=\"train_auc\", color = \"midnightblue\")\n","ax[0].plot(vggtf_valauc, label=\"test_auc\", color = \"salmon\")\n","ax[0].set_xlabel(\"Epoch\")\n","ax[0].legend()\n","\n","ax[1].plot(vggtf_trainloss, label=\"train_loss\", color = \"midnightblue\")\n","ax[1].plot(vggtf_valloss, label=\"test_loss\", color = \"salmon\")\n","ax[1].set_xlabel(\"Epoch\")\n","ax[1].legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aNbSpkb5LNQ0"},"source":["##### Gradcam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_NJWB5VLNQ0","outputId":"639a2c0f-4671-41aa-f554-144f2f47e4be"},"outputs":[],"source":["grad_heatmap(vgg16tf, X_aug, y_aug_new, \"block5_conv3\", dim=(7,10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0uWUtmrLNQ0","outputId":"69120dce-9be3-4e50-d9cd-6393ad4c3ba2"},"outputs":[],"source":["grad_heatmap(vgg16tf, test_X, test_y_new, \"block5_conv3\", dim=(4,10))"]},{"cell_type":"markdown","metadata":{"id":"4kjOwr5CLNQ0"},"source":["#### <span style = \"font-family: Cambria\">VGG16TF_11 - Training layer 11 onwards</span>"]},{"cell_type":"markdown","metadata":{"id":"0rOeHknILNQ0"},"source":["##### Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uqHH7vFLNQ0","outputId":"32cf3ade-11ac-4c92-d374-9b343dc895bd"},"outputs":[],"source":["base_filepath = f\"./tf_exp_6apr/vgg16tf_11_base.h5\"\n","ft_filepath = f\"./tf_exp_6apr/vgg16tf_11_ft.h5\"\n","layer_num = 9\n","\n","batch_size = 64\n","epochs = 15\n","epochs_finetune = 7\n","\n","\n","vgg16tf_11 = create_vgg16tf()\n","\n","# Augmenting train data\n","X_aug, y_aug = augment_minority_class(X, y)\n","y_aug_new = np.array([[1,0] if l == 0 else [0,1] for l in y_aug])\n","\n","train_generator = train_datagen.flow(\n","    x=X_aug,\n","    y=y_aug_new,\n","    batch_size=batch_size)\n","\n","# Test data\n","test_X_new = np.array(test_X)\n","test_y_new = np.array([[1,0] if l == 0 else [0,1] for l in test_y])\n","\n","test_datagen = ImageDataGenerator(\n","    samplewise_center = True,\n","    samplewise_std_normalization = True)\n","\n","test_generator = test_datagen.flow(\n","    x=test_X_new,\n","    y=test_y_new,\n","    shuffle=False,\n","    batch_size=batch_size)\n","\n","optimizer = Adam(learning_rate=0.0005)\n","\n","# Initial learning rate\n","vgg16tf_11.compile(optimizer=optimizer,\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"binary_accuracy\", \"AUC\"])\n","\n","early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, start_from_epoch=0)\n","\n","epoch_history = vgg16tf_11.fit(\n","    train_generator,\n","    epochs=epochs,\n","    validation_data=test_generator,\n","    callbacks=[early_stopping],\n","    verbose=1)\n","\n","# Interim result\n","loss, accuracy, auc = vgg16tf_11.evaluate(test_generator)\n","preds = vgg16tf_11.predict(test_generator)\n","print(added_metrics(preds, test_y_new))\n","\n","# Saving weights of base model\n","vgg16tf_11.save_weights(base_filepath)\n","\n","vggtf_11_trainloss = epoch_history.history[\"loss\"].copy()\n","vggtf_11_trainauc = epoch_history.history[\"auc\"].copy()\n","vggtf_11_valloss = epoch_history.history[\"val_loss\"].copy()\n","vggtf_11_valauc = epoch_history.history[\"val_auc\"].copy()\n","n = len(epoch_history.history[\"loss\"])\n","\n","# Unfrozen layers 11 onwards for finetuning\n","for id, layer in enumerate(vgg16tf_11.layers):\n","    layer.trainable = False\n","    if id > layer_num:\n","        layer.trainable = True\n","\n","optimizer.learning_rate.assign(1e-5)\n","\n","epoch_history_ft = vgg16tf_11.fit(\n","        train_generator,\n","        epochs=epochs_finetune,\n","        validation_data=test_generator,\n","        callbacks=[early_stopping],\n","        verbose=1)\n","\n","vggtf_11_trainloss.extend(epoch_history_ft.history[\"loss\"].copy())\n","vggtf_11_trainauc.extend(epoch_history_ft.history[\"auc\"].copy())\n","vggtf_11_valloss.extend(epoch_history_ft.history[\"val_loss\"].copy())\n","vggtf_11_valauc.extend(epoch_history_ft.history[\"val_auc\"].copy())\n","\n","# Finetuned results\n","loss, accuracy, auc = vgg16tf_11.evaluate(test_generator)\n","preds = vgg16tf_11.predict(test_generator)\n","print(added_metrics(preds, test_y_new))\n","\n","# Saving weights of finetune model\n","vgg16tf_11.save_weights(f\"./tfexp/vgg16tf_11_ft.h5\")\n","\n","# History.history plots\n","fig, ax = plt.subplots(1,2,figsize=(13,6))\n","\n","ax[0].plot(vggtf_11_trainauc, label=\"train_auc\", color=\"midnightblue\")\n","ax[0].plot(vggtf_11_valauc, label=\"test_auc\", color=\"salmon\")\n","ax[0].axvline(x=n, ymin=0, ymax=10, color=\"midnightblue\", alpha=0.5, linestyle=\"--\")\n","ax[0].set_xlabel(\"Epoch\")\n","ax[0].legend()\n","\n","ax[1].plot(vggtf_11_trainloss, label=\"train_loss\", color=\"midnightblue\")\n","ax[1].plot(vggtf_11_valloss, label=\"test_loss\", color=\"salmon\")\n","ax[1].axvline(x=n, ymin=0, ymax=10, color=\"midnightblue\", alpha=0.5, linestyle=\"--\")\n","ax[1].set_xlabel(\"Epoch\")\n","ax[1].legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"KVVQC4ahLNQ0"},"source":["##### Gradcam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4IhsFEaLNQ0","outputId":"273fbb95-9543-4681-8ca5-14be3737947c"},"outputs":[],"source":["grad_heatmap(vgg16tf_11, X_aug, y_aug_new, \"block5_conv3\", dim=(7,10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YAPz7vILNQ0","outputId":"42e842b8-213c-4958-fbd8-53013d35ef39"},"outputs":[],"source":["grad_heatmap(vgg16tf_11, test_X, test_y_new, \"block5_conv3\", dim=(4,10))"]},{"cell_type":"markdown","metadata":{"id":"CoyFy-xZLNQ1"},"source":["#### <span style = \"font-family: Cambria\">VGG16TF_15 - Training layer 15 onwards</span>"]},{"cell_type":"markdown","metadata":{"id":"TVML-cotLNQ1"},"source":["##### Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itWtKLGhLNQ1","outputId":"dcfe43c3-f8c2-4e58-cd06-46b1dc4e14c3"},"outputs":[],"source":["base_filepath = f\"./tf_exp_6apr/vgg16tf_15_base.h5\"\n","ft_filepath = f\"./tf_exp_6apr/vgg16tf_15_ft.h5\"\n","layer_num = 13\n","\n","batch_size = 64\n","epochs = 15\n","epochs_finetune = 7\n","\n","\n","vgg16tf_15 = create_vgg16tf()\n","\n","# Augmenting train data\n","X_aug, y_aug = augment_minority_class(X, y)\n","y_aug_new = np.array([[1,0] if l == 0 else [0,1] for l in y_aug])\n","\n","train_generator = train_datagen.flow(\n","    x=X_aug,\n","    y=y_aug_new,\n","    batch_size=batch_size)\n","\n","# Test data\n","test_X_new = np.array(test_X)\n","test_y_new = np.array([[1,0] if l == 0 else [0,1] for l in test_y])\n","\n","test_datagen = ImageDataGenerator(\n","    samplewise_center = True,\n","    samplewise_std_normalization = True)\n","\n","test_generator = test_datagen.flow(\n","    x=test_X_new,\n","    y=test_y_new,\n","    shuffle=False,\n","    batch_size=batch_size)\n","\n","optimizer = Adam(learning_rate=0.0005)\n","\n","# Initial learning rate\n","vgg16tf_15.compile(optimizer=optimizer,\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"binary_accuracy\", \"AUC\"])\n","\n","early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, start_from_epoch=0)\n","\n","epoch_history = vgg16tf_15.fit(\n","    train_generator,\n","    epochs=epochs,\n","    validation_data=test_generator,\n","    callbacks=[early_stopping],\n","    verbose=1)\n","\n","# Interim result\n","loss, accuracy, auc = vgg16tf_15.evaluate(test_generator)\n","preds = vgg16tf_15.predict(test_generator)\n","print(added_metrics(preds, test_y_new))\n","\n","# Saving weights of base model\n","vgg16tf_15.save_weights(base_filepath)\n","\n","vggtf_15_trainloss = epoch_history.history[\"loss\"].copy()\n","vggtf_15_trainauc = epoch_history.history[\"auc\"].copy()\n","vggtf_15_valloss = epoch_history.history[\"val_loss\"].copy()\n","vggtf_15_valauc = epoch_history.history[\"val_auc\"].copy()\n","n = len(epoch_history.history[\"loss\"])\n","\n","# Unfrozen layers 15 onwards for finetuning\n","for id, layer in enumerate(vgg16tf_15.layers):\n","    layer.trainable = False\n","    if id > layer_num:\n","        layer.trainable = True\n","\n","optimizer.learning_rate.assign(1e-5)\n","\n","epoch_history_ft = vgg16tf_15.fit(\n","        train_generator,\n","        epochs=epochs_finetune,\n","        validation_data=test_generator,\n","        callbacks=[early_stopping],\n","        verbose=1)\n","\n","vggtf_15_trainloss.extend(epoch_history_ft.history[\"loss\"].copy())\n","vggtf_15_trainauc.extend(epoch_history_ft.history[\"auc\"].copy())\n","vggtf_15_valloss.extend(epoch_history_ft.history[\"val_loss\"].copy())\n","vggtf_15_valauc.extend(epoch_history_ft.history[\"val_auc\"].copy())\n","\n","# Finetuned results\n","loss, accuracy, auc = vgg16tf_15.evaluate(test_generator)\n","preds = vgg16tf_15.predict(test_generator)\n","print(added_metrics(preds, test_y_new))\n","\n","# Saving weights of finetune model\n","vgg16tf_15.save_weights(f\"./tfexp/vgg16tf_15_ft.h5\")\n","\n","# History.history plots\n","fig, ax = plt.subplots(1,2,figsize=(13,6))\n","\n","ax[0].plot(vggtf_15_trainauc, label=\"train_auc\", color=\"midnightblue\")\n","ax[0].plot(vggtf_15_valauc, label=\"test_auc\", color=\"salmon\")\n","ax[0].axvline(x=n, ymin=0, ymax=10, color=\"midnightblue\", alpha=0.5, linestyle=\"--\")\n","ax[0].set_xlabel(\"Epoch\")\n","ax[0].legend()\n","\n","ax[1].plot(vggtf_15_trainloss, label=\"train_loss\", color=\"midnightblue\")\n","ax[1].plot(vggtf_15_valloss, label=\"test_loss\", color=\"salmon\")\n","ax[1].axvline(x=n, ymin=0, ymax=10, color=\"midnightblue\", alpha=0.5, linestyle=\"--\")\n","ax[1].set_xlabel(\"Epoch\")\n","ax[1].legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"9bT0CPejLNQ1"},"source":["##### Gradcam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RadQCmzILNQ1","outputId":"8259f3de-d2de-4f51-868d-ed6bae6bc6ab"},"outputs":[],"source":["grad_heatmap(vgg16tf_15, X_aug, y_aug_new, \"block5_conv3\", dim=(7,10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tl0EI2ziLNQ1","outputId":"e63fc344-cdf7-40a1-9dbe-aff5a09d8c91"},"outputs":[],"source":["grad_heatmap(vgg16tf_15, test_X, test_y_new, \"block5_conv3\", dim=(4,10))"]},{"cell_type":"markdown","metadata":{"id":"YY1NVcPCLNQ1"},"source":["#### <span style = \"font-family: Cambria\">VGG16TF_18 - Training layer 18</span>"]},{"cell_type":"markdown","metadata":{"id":"Z8o4-W5lLNQ1"},"source":["##### Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zU-MZ-sALNQ1","outputId":"545e4a6e-3220-4171-f46b-761093b5c516"},"outputs":[],"source":["base_filepath = f\"./tf_exp_6apr/VGG16TF_18_base.h5\"\n","ft_filepath = f\"./tf_exp_6apr/VGG16TF_18_ft.h5\"\n","layer_num = 16\n","\n","batch_size = 64\n","epochs = 15\n","epochs_finetune = 7\n","\n","vgg16tf_18 = create_vgg16tf()\n","\n","# Augmenting train data\n","X_aug, y_aug = augment_minority_class(X, y)\n","y_aug_new = np.array([[1,0] if l == 0 else [0,1] for l in y_aug])\n","\n","train_generator = train_datagen.flow(\n","    x=X_aug,\n","    y=y_aug_new,\n","    batch_size=batch_size)\n","\n","# Test data\n","test_X_new = np.array(test_X)\n","test_y_new = np.array([[1,0] if l == 0 else [0,1] for l in test_y])\n","\n","test_datagen = ImageDataGenerator(\n","    samplewise_center = True,\n","    samplewise_std_normalization = True)\n","\n","test_generator = test_datagen.flow(\n","    x=test_X_new,\n","    y=test_y_new,\n","    shuffle=False,\n","    batch_size=batch_size)\n","\n","optimizer = Adam(learning_rate=0.0005)\n","\n","# Initial learning rate\n","vgg16tf_18.compile(optimizer=optimizer,\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"binary_accuracy\", \"AUC\"])\n","\n","early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, start_from_epoch=0)\n","\n","epoch_history = vgg16tf_18.fit(\n","    train_generator,\n","    epochs=epochs,\n","    validation_data=test_generator,\n","    callbacks=[early_stopping],\n","    verbose=1)\n","\n","# Interim result\n","loss, accuracy, auc = vgg16tf_18.evaluate(test_generator)\n","preds = vgg16tf_18.predict(test_generator)\n","print(added_metrics(preds, test_y_new))\n","\n","# Saving weights of base model\n","vgg16tf_18.save_weights(base_filepath)\n","\n","vggtf_18_trainloss = epoch_history.history[\"loss\"].copy()\n","vggtf_18_trainauc = epoch_history.history[\"auc\"].copy()\n","vggtf_18_valloss = epoch_history.history[\"val_loss\"].copy()\n","vggtf_18_valauc = epoch_history.history[\"val_auc\"].copy()\n","n = len(epoch_history.history[\"loss\"])\n","\n","# Unfrozen layers 18 onwards for finetuning\n","for id, layer in enumerate(vgg16tf_18.layers):\n","    layer.trainable = False\n","    if id > layer_num:\n","        layer.trainable = True\n","\n","optimizer.learning_rate.assign(1e-5)\n","\n","epoch_history_ft = vgg16tf_18.fit(\n","        train_generator,\n","        epochs=epochs_finetune,\n","        validation_data=test_generator,\n","        callbacks=[early_stopping],\n","        verbose=1)\n","\n","vggtf_18_trainloss.extend(epoch_history_ft.history[\"loss\"].copy())\n","vggtf_18_trainauc.extend(epoch_history_ft.history[\"auc\"].copy())\n","vggtf_18_valloss.extend(epoch_history_ft.history[\"val_loss\"].copy())\n","vggtf_18_valauc.extend(epoch_history_ft.history[\"val_auc\"].copy())\n","\n","# Finetuned results\n","loss, accuracy, auc = vgg16tf_18.evaluate(test_generator)\n","preds = vgg16tf_18.predict(test_generator)\n","\n","print(added_metrics(preds, test_y_new))\n","\n","# Saving weights of finetune model\n","vgg16tf_18.save_weights(f\"./tfexp/vgg16tf_18_ft.h5\")\n","\n","# History.history plots\n","fig, ax = plt.subplots(1,2,figsize=(13,6))\n","\n","ax[0].plot(vggtf_18_trainauc, label=\"train_auc\", color=\"midnightblue\")\n","ax[0].plot(vggtf_18_valauc, label=\"test_auc\", color=\"salmon\")\n","ax[0].axvline(x=n, ymin=0, ymax=10, color=\"midnightblue\", alpha=0.5, linestyle=\"--\")\n","ax[0].set_xlabel(\"Epoch\")\n","ax[0].legend()\n","\n","ax[1].plot(vggtf_18_trainloss, label=\"train_loss\", color=\"midnightblue\")\n","ax[1].plot(vggtf_18_valloss, label=\"test_loss\", color=\"salmon\")\n","ax[1].axvline(x=n, ymin=0, ymax=10, color=\"midnightblue\", alpha=0.5, linestyle=\"--\")\n","ax[1].set_xlabel(\"Epoch\")\n","ax[1].legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"RnxhfJKYLNQ1"},"source":["##### Gradcam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9NOpD7zLNQ1","outputId":"07301548-0a4a-41ce-bdf3-bade2e93196a"},"outputs":[],"source":["grad_heatmap(vgg16tf_18, X_aug, y_aug_new, \"block5_conv3\", dim=(7,10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGh383QELNQ1","outputId":"93c6c84b-a582-496c-f882-e770a2d035ea"},"outputs":[],"source":["grad_heatmap(vgg16tf_18, test_X, test_y_new, \"block5_conv3\", dim=(4,10))"]},{"cell_type":"markdown","metadata":{"id":"N3rFoH8bLNQ1"},"source":["# <a id = \"xgboost\"></a><span style = \"font-family: Cambria\">**VGG16-XgBoost model**</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Z5az5wDLNQ1"},"outputs":[],"source":["# Data Splitting\n","benign_train_image, benign_test_image, benign_train_image_label, benign_test_image_label = train_test_split(b_imgs, b_label, test_size = 0.2)\n","malignant_train_image, malignant_test_image, malignant_train_image_label, malignant_test_image_label = train_test_split(m_imgs, m_label, test_size = 0.1)\n","\n","train_image = np.append(benign_train_image, malignant_train_image, axis = 0)\n","test_image = np.append(benign_test_image, malignant_test_image, axis = 0)\n","train_label = np.append(benign_train_image_label, malignant_train_image_label, axis = 0)\n","test_label = np.append(benign_test_image_label, malignant_test_image_label, axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbPiKiZaLNQ1","outputId":"d85acd43-0182-4e1f-8c7b-f40b94cddaf4"},"outputs":[],"source":["print(np.shape(train_image))\n","print(np.shape(test_image))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksckx6l-LNQ1"},"outputs":[],"source":["# Data Augmentation\n","augmented_benign_train_image = augment_images_xgboost(benign_train_image, 600, seed = random.randint(1, 2024))\n","augmented_malignant_train_image = augment_images_xgboost(malignant_train_image, 600, seed = random.randint(1, 2024))\n","\n","train_image = np.append(augmented_benign_train_image, augmented_malignant_train_image, axis = 0)\n","\n","benign_labels = np.zeros((len(augmented_benign_train_image),))\n","malignant_labels = np.ones((len(augmented_malignant_train_image),))\n","\n","train_label = np.append(benign_labels, malignant_labels, axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAQobtD-LNQ1","outputId":"ee0dd82d-43ed-4499-a733-5274d00c729b"},"outputs":[],"source":["print(np.shape(augmented_benign_train_image))\n","print(np.shape(augmented_malignant_train_image))\n","print(np.shape(train_label))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jficQ-lTLNQ1","outputId":"a7951249-4cfb-4e4b-e032-c692849173d5"},"outputs":[],"source":["print(np.shape(train_image))\n","print(np.shape(test_image))\n","print(np.shape(train_label))\n","print(np.shape(test_label))"]},{"cell_type":"markdown","metadata":{"id":"ZzLIvgu8LNQ2"},"source":["## <span style = \"font-family: Cambria\">CNN for Feature Extraction by CNN </span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8o5n2NXLNQ2","outputId":"da277dd8-4423-4e75-f36a-c6aca63890d1"},"outputs":[],"source":["feature_extractor_model = VGG16(weights = 'imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","for layer in feature_extractor_model.layers:\n","\tlayer.trainable = False\n","\n","train_features = feature_extractor_model.predict(train_image)\n","reshaped_train_features = train_features.reshape(train_features.shape[0], -1)\n","\n","test_features = feature_extractor_model.predict(test_image)\n","reshaped_test_features = test_features.reshape(test_features.shape[0], -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BX7_T8g5LNQ2","outputId":"5f6b6c45-335f-47e2-84ef-bc56091df9a1"},"outputs":[],"source":["print(np.shape(reshaped_train_features))\n","print(np.shape(reshaped_test_features))"]},{"cell_type":"markdown","metadata":{"id":"JmrlsHWlLNQ2"},"source":["## <span style = \"font-family: Cambria\">PCA Model </span>"]},{"cell_type":"markdown","metadata":{"id":"ff_VguC-LNQ2"},"source":["We explored PCA to reduce the dimensions of flatten features from CNN while still retaining variability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBY4GdUqLNQ2","outputId":"6639b72b-6e12-4663-cd84-d9ef32f077c3"},"outputs":[],"source":["pca_model = PCA(n_components = 1000)\n","feature_pca = pca_model.fit_transform(reshaped_train_features)\n","\n","fig = plt.figure(figsize=(10, 8))\n","ax = fig.add_subplot(111, projection='3d')\n","scatter = ax.scatter(feature_pca[:, 0], feature_pca[:, 1], feature_pca[:, 2], c = train_label, cmap='viridis')\n","plt.colorbar(scatter, label='Digit', ticks=range(10))\n","ax.set_xlabel('Principal Component 1')\n","ax.set_ylabel('Principal Component 2')\n","ax.set_zlabel('Principal Component 3')\n","ax.view_init(30, 75)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPlUQ2CHLNQ2","outputId":"efff221a-b2b9-49f3-f7e5-7aa2bcaab3f1"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.bar(range(1, len(pca_model.explained_variance_ratio_) + 1), pca_model.explained_variance_ratio_, alpha=0.5, align='center')\n","plt.step(range(1, len(pca_model.explained_variance_ratio_) + 1), np.cumsum(pca_model.explained_variance_ratio_), where='mid')\n","plt.ylabel('Explained variance ratio')\n","plt.xlabel('Principal components')\n","plt.title('Explained Variance vs. Number of Principal Components')\n","plt.axhline(y=0.8, color='r', linestyle='--', label='Threshold (0.8)') # Shown at approx. 250\n","plt.axhline(y=0.9, color='b', linestyle='--', label='Threshold (0.9)') # Shown at approx. 450\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycl_6aV4LNQ2","outputId":"02b1b98b-8dd9-4a39-b69b-e66cc810b8a6"},"outputs":[],"source":["model = SVC(kernel='linear')\n","clf = model.fit(reshaped_train_features, train_label)\n","\n","z = lambda x,y: (clf.coef_[0][0]*x +clf.coef_[0][1]*y) / (clf.coef_[0][2]+0.1)\n","\n","# Plot the decision plane in 3D\n","fig = plt.figure(figsize=(10, 8))\n","ax = fig.add_subplot(111, projection='3d')\n","scatter = ax.scatter(feature_pca[:, 0], feature_pca[:, 2], feature_pca[:, 1], c = train_label, cmap='viridis')\n","plt.colorbar(scatter, label='Digit', ticks=range(10))\n","ax.set_xlabel('Principal Component 2')\n","ax.set_ylabel('Principal Component 3')\n","ax.set_zlabel('Principal Component 1')\n","\n","tmp = np.linspace(-400, 400,10)\n","x,y = np.meshgrid(tmp,tmp)\n","# Plot the decision plane surface\n","ax.plot_surface(x, y, z(x,y))\n","ax.view_init(20, 45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9M3uZlbDLNQ2"},"source":["This shows how PCA could make a significant distinction across the images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aBMUpR8lLNQ2"},"outputs":[],"source":["# Final PCA Model\n","\n","pca_model = PCA(n_components = 450)\n","reduced_features_450 = pca_model.fit_transform(reshaped_train_features)\n","reduced_features_250 = reduced_features_450[:, :250]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OAp8PhgLNQ2","outputId":"49f63c8e-03d4-48e5-b926-f9cb12155de5"},"outputs":[],"source":["print(np.shape(reduced_features_450))\n","print(np.shape(reduced_features_250))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWoPtFdVLNQ2"},"outputs":[],"source":["test_features_450 = pca_model.transform(reshaped_test_features)\n","test_features_250 = test_features_450[:,:250]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeTZPsmdLNQ2","outputId":"90c2dcd7-afe2-44ec-a231-0439cc80deae"},"outputs":[],"source":["print(np.shape(test_features_450))\n","print(np.shape(test_features_250))"]},{"cell_type":"markdown","metadata":{"id":"bKR5SkYtLNQ2"},"source":["## <span style = \"font-family: Cambria\">XGBoost using the extracted features</span>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOz_NodELNQ3"},"outputs":[],"source":["def data_transform(features, labels):\n","  matrix = XGBoost.DMatrix(features, label = labels)\n","  return matrix\n","\n","\n","def model_fitting(model, features, labels):\n","\n","    xgb_param = model.get_xgb_params()\n","    xgtrain = data_transform(features, labels = labels)\n","    cvresult = XGBoost.cv(xgb_param, xgtrain, num_boost_round=model.get_params()['n_estimators'], nfold = 5,\n","            metrics='auc', early_stopping_rounds = 20)\n","    model.set_params(n_estimators = cvresult.shape[0])\n","\n","    #Fit the algorithm on the data\n","    model.fit(features, labels, eval_metric='auc')\n","\n","    #Predict training set:\n","    dtrain_predictions = model.predict(features)\n","    dtrain_predprob = model.predict_proba(features)[:,1]\n","\n","    #Print model report:\n","    print (\"\\nModel Report\")\n","    print (\"Accuracy : \")\n","    print(sklearn.metrics.accuracy_score(labels, dtrain_predictions))\n","    print(\" \")\n","    print (\"AUC Score (Train): \")\n","    print(sklearn.metrics.roc_auc_score(labels, dtrain_predprob))\n","\n","    feat_imp = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n","    feat_imp.plot(kind = 'bar', title='Feature Importances')\n","    plt.ylabel('Feature Importance Score')"]},{"cell_type":"markdown","metadata":{"id":"lq6mnzm5LNQ3"},"source":["### <span style = \"font-family: Cambria\">Initializing XGBoost</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QoNooRqfLNQ3"},"outputs":[],"source":["initial_XGBoost = XGBoost.XGBClassifier(learning_rate = 0.1,\n","                                n_estimators = 1000,\n","                                max_depth = 5,\n","                                min_child_weight = 1,\n","                                gamma = 0,\n","                                subsample = 0.8,\n","                                colsample_bytree = 0.8,\n","                                objective = 'binary:logistic',\n","                                nthread = 4,\n","                                scale_pos_weight = 1,\n","                                seed = 42)"]},{"cell_type":"markdown","metadata":{"id":"q1ZbK4vMLNQ3"},"source":["### <span style = \"font-family: Cambria\">1. Searching for the best n_estimators</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBNjlYiXLNQ3"},"outputs":[],"source":["xgb_param = initial_XGBoost.get_xgb_params()\n","xgtrain = data_transform(reshaped_train_features, labels = train_label)\n","cvresult = XGBoost.cv(xgb_param, xgtrain, num_boost_round = initial_XGBoost.get_params()['n_estimators'], nfold = 5,\n","            metrics='auc', early_stopping_rounds = 20)\n","initial_XGBoost.set_params(n_estimators = cvresult.shape[0])\n","\n","#Fit the algorithm on the data\n","initial_XGBoost.fit(reshaped_train_features, train_label, eval_metric='auc')\n","\n","print(cvresult.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"8i0OOuzuLNQ3"},"source":["The result of the best number of trees (n_estimators) is 449"]},{"cell_type":"markdown","metadata":{"id":"lYwyEdgrLNQ3"},"source":["### <span style = \"font-family: Cambria\">2. Tuning max_depth and min_child_weight </span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgZoZhpFLNQ3"},"outputs":[],"source":["second_XGBoost = XGBoost.XGBClassifier(learning_rate = 0.1,\n","                                                  n_estimators = 449,\n","                                                  max_depth=5,\n","                                                  min_child_weight=1,\n","                                                  gamma=0,\n","                                                  subsample=0.8,\n","                                                  colsample_bytree=0.8,\n","                                                  objective= 'binary:logistic',\n","                                                  nthread=4,\n","                                                  scale_pos_weight=1,\n","                                                  seed = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArfZMKJQLNQ3"},"outputs":[],"source":["param_grid_2 = {\n"," 'max_depth':range(3,10,1),\n"," 'min_child_weight':range(1,6,1)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGpgY7rrLNQ3"},"outputs":[],"source":["grid_search_2 = GridSearchCV(second_XGBoost, param_grid = param_grid_2, scoring = 'roc_auc', n_jobs = 1 , cv = 5, verbose = 2)\n","\n","grid_search_2.fit(reshaped_train_features,train_label)\n","print(grid_search_2.best_estimator_)\n","print(grid_search_2.best_params_)\n","print(grid_search_2.best_score_)"]},{"cell_type":"markdown","metadata":{"id":"o7xKtXojLNQ3"},"source":["The best parameters for both max_depth and min_child_weight is 3 and 1 respectively.\n","\n","The best auc score for this classifier is 0.9921755829903978"]},{"cell_type":"markdown","metadata":{"id":"P3Kcp6hwLNQ3"},"source":["### <span style = \"font-family: Cambria\">3. Tuning Gamma</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1B3Qa3R5LNQ3"},"outputs":[],"source":["third_XGBoost = XGBoost.XGBClassifier(learning_rate = 0.1,\n","                                                  n_estimators = 449,\n","                                                  max_depth=3,\n","                                                  min_child_weight=1,\n","                                                  gamma=0,\n","                                                  subsample=0.8,\n","                                                  colsample_bytree=0.8,\n","                                                  objective= 'binary:logistic',\n","                                                  nthread=4,\n","                                                  scale_pos_weight=1,\n","                                                  seed = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MXaHVRgLNQ4"},"outputs":[],"source":["param_grid_3 = {\n"," 'gamma':[i/10.0 for i in range(0,11)]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_xU_lOtLNQ4"},"outputs":[],"source":["grid_search_3 = GridSearchCV(third_XGBoost, param_grid = param_grid_3, scoring = 'roc_auc', n_jobs = 1 , cv = 5, verbose = 2)\n","\n","grid_search_3.fit(reshaped_train_features,train_label)\n","print(grid_search_3.best_estimator_)\n","print(grid_search_3.best_params_)\n","print(grid_search_3.best_score_)"]},{"cell_type":"markdown","metadata":{"id":"fM2Jd1RfLNQ4"},"source":["Best gamma is obtained when gamma = 0.1 with a total auc score of 0.9931851851851852"]},{"cell_type":"markdown","metadata":{"id":"N5jvBQw6LNQ4"},"source":["### <span style = \"font-family: Cambria\">4. Tuning subsample and colsample_by_tree</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrxWZ31PLNQ4"},"outputs":[],"source":["fourth_XGBoost = XGBoost.XGBClassifier(learning_rate = 0.1,\n","                                                  n_estimators = 449,\n","                                                  max_depth=3,\n","                                                  min_child_weight=1,\n","                                                  gamma=0.1,\n","                                                  subsample=0.8,\n","                                                  colsample_bytree=0.8,\n","                                                  objective= 'binary:logistic',\n","                                                  nthread=4,\n","                                                  scale_pos_weight=1,\n","                                                  seed = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"To94C0eDLNQ4"},"outputs":[],"source":["param_grid_4 = {\n"," 'subsample':[i/10.0 for i in range(5,11)],\n"," 'colsample_bytree':[i/10.0 for i in range(5,11)]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FiKRqbRxLNQ4","outputId":"1e65c69d-50ff-43f9-ac75-f5217c6014cd"},"outputs":[],"source":["grid_search_4 = GridSearchCV(fourth_XGBoost, param_grid = param_grid_4, scoring = 'roc_auc', n_jobs = 5, cv = 5, verbose = 2)\n","\n","grid_search_4.fit(reshaped_train_features,train_label)\n","print(grid_search_4.best_estimator_)\n","print(grid_search_4.best_params_)\n","print(grid_search_4.best_score_)"]},{"cell_type":"markdown","metadata":{"id":"Q3J3tiBVLNQ4"},"source":["Best colsample_bytree and subsample is 0.6 and 0.6 respectively, with a final auc score of 0.9932181069958848"]},{"cell_type":"markdown","metadata":{"id":"VYwI3dbHLNQ4"},"source":["### <span style = \"font-family: Cambria\">5. Tuning the regularization method</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gV69Q33LNQ4"},"outputs":[],"source":["fifth_XGBoost = XGBoost.XGBClassifier(learning_rate = 0.1,\n","                                                  n_estimators = 449,\n","                                                  max_depth=3,\n","                                                  min_child_weight=1,\n","                                                  gamma=0.1,\n","                                                  subsample=0.6,\n","                                                  colsample_bytree=0.6,\n","                                                  objective= 'binary:logistic',\n","                                                  nthread=4,\n","                                                  scale_pos_weight=1,\n","                                                  seed = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbxQcc7HLNQ4"},"outputs":[],"source":["param_grid_5 = {\n"," 'reg_alpha':[1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBLUmZj1LNQ4"},"outputs":[],"source":["grid_search_5 = GridSearchCV(fifth_XGBoost, param_grid = param_grid_5, scoring = 'roc_auc', n_jobs = 1 , cv = 5, verbose = 2)\n","\n","grid_search_5.fit(reshaped_train_features,train_label)\n","print(grid_search_5.best_estimator_)\n","print(grid_search_5.best_params_)\n","print(grid_search_5.best_score_)\n","\n","print(grid_searh_5.best_params_[\"reg_alpha\"])"]},{"cell_type":"markdown","metadata":{"id":"jrRsSgpOLNQ5"},"source":["Best reg_alpha is 0.1 with the final auc_score 0.9934156378600824"]},{"cell_type":"markdown","metadata":{"id":"PLhVnX_tLNQ5"},"source":["### <span style = \"font-family: Cambria\">6. Minimizing learning rate, and adjusting number of boosting rounds for better results</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6exN8h6iLNQ5","outputId":"9bc3309d-9840-4b1b-f686-3291096688aa"},"outputs":[],"source":["sixth_XGBoost = XGBoost.XGBClassifier(learning_rate = 0.01,\n","                                                  n_estimators = 449,\n","                                                  max_depth=3,\n","                                                  min_child_weight=1,\n","                                                  gamma=0.1,\n","                                                  subsample=0.6, # change this\n","                                                  colsample_bytree=0.6, #change this\n","                                                  objective= 'binary:logistic',\n","                                                  nthread=4,\n","                                                  scale_pos_weight=1,\n","                                                  reg_alpha = 0.1,\n","                                                  seed = 42) # then add reg_alpha"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFPfOZc0LNQ5","outputId":"aeabd9fe-847c-4180-cb63-7a002adfc41f"},"outputs":[],"source":["xgb_param = sixth_XGBoost.get_xgb_params()\n","xgtrain = data_transform(reshaped_train_features, labels = train_label)\n","cvresult = XGBoost.cv(xgb_param, xgtrain, num_boost_round = sixth_XGBoost.get_params()['n_estimators'], nfold = 5,\n","            metrics='auc', early_stopping_rounds = 20)\n","sixth_XGBoost.set_params(n_estimators = cvresult.shape[0], eval_metric='auc')\n","\n","#Fit the algorithm on the data\n","sixth_XGBoost.fit(reshaped_train_features, train_label)\n","\n","print(cvresult.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"QBlpInoqLNQ5"},"source":["Finally, this is the end of the tuning algorithm and from the last algo, the best number of estimators are 449."]},{"cell_type":"markdown","metadata":{"id":"4BqkSqyjLNQ5"},"source":["## <span style = \"font-family: Cambria\">Performance Testing</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tAwvutxLNQ5","outputId":"ccc02d67-d37b-4bf4-b123-6d83b3c840c1"},"outputs":[],"source":["test_features = reshaped_test_features\n","test_labels = test_label\n","\n","prediction = sixth_XGBoost.predict_proba(test_features)[:,1]\n","\n","roc_auc = sklearn.metrics.roc_auc_score(test_labels, prediction)\n","fpr, tpr, _ = sklearn.metrics.roc_curve(test_labels, prediction)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2cKrx3ULNQ5","outputId":"83c79082-fb60-4d00-b1e0-d8281014f540"},"outputs":[],"source":["prediction_class = sixth_XGBoost.predict(test_features)\n","tn, fp, fn, tp = sklearn.metrics.confusion_matrix(test_label, prediction_class).ravel()\n","\n","# Calculate Precision, Recall, and Accuracy\n","precision = sklearn.metrics.precision_score(test_label, prediction_class)\n","accuracy = sklearn.metrics.accuracy_score(test_label, prediction_class)\n","recall = sklearn.metrics.recall_score(test_label, prediction_class)\n","f1_score = sklearn.metrics.f1_score(test_label, prediction_class)\n","\n","print(\"F1_score:\" ,f1_score)\n","\n","# Calculate ROC AUC Score\n","auc = sklearn.metrics.roc_auc_score(test_label, prediction_class)\n","\n","\n","print(\"True Negatives:\", tn)\n","print(\"False Positives:\", fp)\n","print(\"False Negatives:\", fn)\n","print(\"True Positives:\", tp)\n","print(\"Precision:\", precision)\n","print(\"Accuracy:\", accuracy)\n","print(\"Recall:\", recall)\n","print(\"AUC:\", auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJhJA_IkLNQ5","outputId":"034569e3-5e84-470f-8215-6aa0d7236a25"},"outputs":[],"source":["XGBoost.plot_tree(sixth_XGBoost)"]},{"cell_type":"markdown","metadata":{"id":"IUzdKnNqLNQ5"},"source":["### <span style = \"font-family: Cambria\">Function to Perform all the hyper tuning above</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPdcWqa4LNQ5"},"outputs":[],"source":["def hyper_parameter_tuning(features, labels, n_jobs = 1, verbose = 0):\n","\n","    model = XGBoost.XGBClassifier(learning_rate = 0.1,\n","                                n_estimators = 1000,\n","                                max_depth = 5,\n","                                min_child_weight = 1,\n","                                gamma = 0,\n","                                subsample = 0.8,\n","                                colsample_bytree = 0.8,\n","                                objective = 'binary:logistic',\n","                                nthread = 4,\n","                                scale_pos_weight = 1,\n","                                seed = 42)\n","\n","    # Step 1.\n","    xgb_param = model.get_xgb_params()\n","    xgtrain = data_transform(features, labels = labels)\n","    cvresult = XGBoost.cv(xgb_param, xgtrain, num_boost_round = model.get_params()['n_estimators'], nfold = 5,\n","            metrics='auc', early_stopping_rounds = 20)\n","\n","    model.set_params(n_estimators = cvresult.shape[0])\n","\n","    print(\"Step 1 Done\")\n","    print(\"Result: Best n_estimators = \" , str(cvresult.shape[0]))\n","\n","    # Step 2.\n","    param_grid_2 = {'max_depth':range(3,10,1),\n","                    'min_child_weight':range(1,6,1)\n","                   }\n","    grid_search_2 = GridSearchCV(model, param_grid = param_grid_2, scoring = 'roc_auc', n_jobs = n_jobs , cv = 5, verbose = verbose)\n","    grid_search_2.fit(features,labels)\n","\n","    model.set_params(max_depth = grid_search_2.best_params_[\"max_depth\"])\n","    model.set_params(min_child_weight = grid_search_2.best_params_[\"min_child_weight\"])\n","\n","    print(\"Step 2 Done\")\n","    print(\"Result: Best max_depth, min_child_weight = \" , str(grid_search_2.best_params_[\"max_depth\"]) , \",\" ,\n","          str(grid_search_2.best_params_[\"min_child_weight\"]))\n","\n","    # Step 3.\n","    param_grid_3 = {'gamma':[i/10.0 for i in range(0,11)]}\n","\n","    grid_search_3 = GridSearchCV(model, param_grid = param_grid_3, scoring = 'roc_auc', n_jobs = n_jobs , cv = 5, verbose = verbose)\n","    grid_search_3.fit(features,labels)\n","\n","    model.set_params(gamma = grid_search_3.best_params_[\"gamma\"])\n","\n","    print(\"Step 3 Done\")\n","    print(\"Result: Best gamma = \" , str(grid_search_3.best_params_[\"gamma\"]))\n","\n","    # Step 4.\n","    param_grid_4 = {'subsample':[i/10.0 for i in range(5,11)],\n","                    'colsample_bytree':[i/10.0 for i in range(5,11)]}\n","\n","    grid_search_4 = GridSearchCV(model, param_grid = param_grid_4, scoring = 'roc_auc', n_jobs = n_jobs , cv = 5, verbose = verbose)\n","    grid_search_4.fit(features,labels)\n","\n","    model.set_params(subsample = grid_search_4.best_params_[\"subsample\"])\n","    model.set_params(colsample_bytree = grid_search_4.best_params_[\"colsample_bytree\"])\n","\n","    print(\"Step 4 Done\")\n","    print(\"Result: Best subsample, colsample_by_tree = \" , str(grid_search_4.best_params_[\"subsample\"]) , \",\" ,\n","          str(grid_search_4.best_params_[\"colsample_bytree\"]))\n","\n","    # Step 5.\n","    param_grid_5 = {'reg_alpha':[1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100]}\n","\n","    grid_search_5 = GridSearchCV(model, param_grid = param_grid_5, scoring = 'roc_auc', n_jobs = n_jobs , cv = 5, verbose = verbose)\n","    grid_search_5.fit(features,labels)\n","\n","    model.set_params(reg_alpha = grid_search_5.best_params_[\"reg_alpha\"])\n","\n","    print(\"Step 5 Done\")\n","    print(\"Result: Best reg_alpha = \" , str(grid_search_5.best_params_[\"reg_alpha\"]))\n","\n","    # Step 6.\n","    xgb_param = model.get_xgb_params()\n","    xgtrain = data_transform(features, labels = labels)\n","    cvresult = XGBoost.cv(xgb_param, xgtrain, num_boost_round = model.get_params()['n_estimators'], nfold = 5,\n","            metrics='auc', early_stopping_rounds = 20)\n","    model.set_params(n_estimators = cvresult.shape[0])\n","\n","    model.fit(features, labels)\n","\n","    print(\"Step 6 Done\")\n","    print(\"Result: Best n_estimators = \" , str(cvresult.shape[0]))\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-I5PQ_1LNQ5","outputId":"780b6bbe-6e64-4837-f459-04b67f4f0c5d"},"outputs":[],"source":["model_pca_450 = hyper_parameter_tuning(reduced_features_450, train_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvCv0t-vLNQ5","outputId":"62d5b5d2-25dd-4d8f-82d0-75f9e287faff"},"outputs":[],"source":["model_pca_250 = hyper_parameter_tuning(reduced_features_250, train_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMxzK3FJLNQ5","outputId":"c4769430-b30b-4a51-f68e-420a00a9472c"},"outputs":[],"source":["test_features = test_features_450\n","test_labels = test_label\n","\n","prediction = model_pca_450.predict_proba(test_features)[:,1]\n","\n","roc_auc = sklearn.metrics.roc_auc_score(test_labels, prediction)\n","fpr, tpr, _ = sklearn.metrics.roc_curve(test_labels, prediction)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8qvm0luLNQ5","outputId":"bdbfe2d4-41e4-4a3f-aa33-0069f32d16cf"},"outputs":[],"source":["prediction_class = model_pca_450.predict(test_features)\n","tn, fp, fn, tp = sklearn.metrics.confusion_matrix(test_label, prediction_class).ravel()\n","\n","# Calculate Precision, Recall, and Accuracy\n","precision = sklearn.metrics.precision_score(test_label, prediction_class)\n","accuracy = sklearn.metrics.accuracy_score(test_label, prediction_class)\n","recall = sklearn.metrics.recall_score(test_label, prediction_class)\n","\n","# Calculate ROC AUC Score\n","auc = sklearn.metrics.roc_auc_score(test_label, prediction_class)\n","\n","print(\"True Negatives:\", tn)\n","print(\"False Positives:\", fp)\n","print(\"False Negatives:\", fn)\n","print(\"True Positives:\", tp)\n","print(\"Precision:\", precision)\n","print(\"Accuracy:\", accuracy)\n","print(\"Recall:\", recall)\n","print(\"AUC:\", auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgKrH5wKLNQ5","outputId":"77891758-5274-4c4d-913d-4e2b78976c22"},"outputs":[],"source":["test_features = test_features_250\n","test_labels = test_label\n","\n","prediction = model_pca_250.predict_proba(test_features)[:,1]\n","\n","roc_auc = sklearn.metrics.roc_auc_score(test_labels, prediction)\n","fpr, tpr, _ = sklearn.metrics.roc_curve(test_labels, prediction)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"te3PWJZiLNQ6","outputId":"91522f5e-42e0-41d9-c117-c25f0f585cb0"},"outputs":[],"source":["prediction_class = model_pca_250.predict(test_features)\n","tn, fp, fn, tp = sklearn.metrics.confusion_matrix(test_label, prediction_class).ravel()\n","\n","# Calculate Precision, Recall, and Accuracy\n","precision = sklearn.metrics.precision_score(test_label, prediction_class)\n","accuracy = sklearn.metrics.accuracy_score(test_label, prediction_class)\n","recall = sklearn.metrics.recall_score(test_label, prediction_class)\n","f1_score = sklearn.metrics.f1_score(test_label, prediction_class)\n","\n","print(\"F1_score:\" ,f1_score)\n","\n","# Calculate ROC AUC Score\n","auc = sklearn.metrics.roc_auc_score(test_label, prediction_class)\n","\n","print(\"True Negatives:\", tn)\n","print(\"False Positives:\", fp)\n","print(\"False Negatives:\", fn)\n","print(\"True Positives:\", tp)\n","print(\"Precision:\", precision)\n","print(\"Accuracy:\", accuracy)\n","print(\"Recall:\", recall)\n","print(\"AUC:\", auc)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
